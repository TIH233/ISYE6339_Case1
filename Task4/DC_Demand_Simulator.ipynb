{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: DC-Level Daily Demand Simulator\n",
    "\n",
    "**Objective**: Generate DC-level daily realized demand from segment-level simulation\n",
    "\n",
    "**Pipeline**:\n",
    "1. Run modified OTD simulator (saves country, segment info)\n",
    "2. Disaggregate segment demand to city-level using population proportions\n",
    "3. Map cities to DCs using Task2 assignment file\n",
    "4. Aggregate to DC-level daily demand\n",
    "\n",
    "**Output**: `(sim, date, year, euro_dc_id, model, realized_units)`\n",
    "\n",
    "**Configuration**:\n",
    "- Simulations: 100\n",
    "- Years: 2027-2034\n",
    "- Adoption scenario: 'mp' (most probable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c041f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from datetime import date, datetime, timedelta\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df886b",
   "metadata": {},
   "source": [
    "## 1. Configuration & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b612e894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "  Simulations: 100\n",
      "  Years: 2027-2034\n",
      "  Adoption scenario: mp\n",
      "  Output directory: ../Task4/dc_output\n",
      "  Country map entries: 29\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "N_SIM = 100\n",
    "SEED = 42\n",
    "YEARS = list(range(2027, 2035))  # 2027-2034\n",
    "ADOPTION_SCENARIO = 'mp'  # most probable\n",
    "BATCH_SIZE = 10  \n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path('../')\n",
    "TASK1_DIR = BASE_DIR / 'Task1'\n",
    "TASK2_DIR = BASE_DIR / 'Task2'\n",
    "TASK4_DIR = BASE_DIR / 'Task4'\n",
    "OUTPUT_DIR = TASK4_DIR / 'dc_output'\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# CONSTANTS (from Task2)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "CYBER_WEEK_SHARE = 0.18\n",
    "CYBER_PRICE_DISCOUNT = 0.15\n",
    "\n",
    "DOW_WEIGHTS = {\n",
    "    0: 0.10,  # Monday\n",
    "    1: 0.12,  # Tuesday\n",
    "    2: 0.13,  # Wednesday\n",
    "    3: 0.14,  # Thursday\n",
    "    4: 0.18,  # Friday\n",
    "    5: 0.19,  # Saturday\n",
    "    6: 0.14,  # Sunday\n",
    "}\n",
    "\n",
    "# Market entry years by country (ISO 2-letter codes)\n",
    "ENTRY_YEAR_MAP = {\n",
    "    'BE': 2027, 'DE': 2027, 'LU': 2027, 'NL': 2027,\n",
    "    'DK': 2028, 'EE': 2028, 'FI': 2028, 'LT': 2028, 'LV': 2028, 'SE': 2028,\n",
    "    'AT': 2029, 'CZ': 2029, 'ES': 2029, 'FR': 2029, 'IT': 2029, 'PL': 2029, 'PT': 2029,\n",
    "    'BG': 2030, 'GR': 2030, 'HR': 2030, 'HU': 2030, 'IE': 2030, 'RO': 2030, 'SI': 2030, 'SK': 2030,\n",
    "}\n",
    "\n",
    "# Full country name → ISO 2-letter code mapping\n",
    "# (Pop_inputs.xlsx uses full names; all other lookups use ISO codes)\n",
    "COUNTRY_NAME_TO_CODE = {\n",
    "    'Austria': 'AT', 'Belgium': 'BE', 'Bulgaria': 'BG', 'Croatia': 'HR',\n",
    "    'Cyprus': 'CY', 'Czech Republic': 'CZ', 'Denmark': 'DK', 'Estonia': 'EE',\n",
    "    'Finland': 'FI', 'France': 'FR', 'Germany': 'DE', 'Greece': 'GR',\n",
    "    'Hungary': 'HU', 'Ireland': 'IE', 'Italy': 'IT', 'Latvia': 'LV',\n",
    "    'Lithuania': 'LT', 'Luxembourg': 'LU', 'Malta': 'MT', 'Netherlands': 'NL',\n",
    "    'Norway': 'NO', 'Poland': 'PL', 'Portugal': 'PT', 'Romania': 'RO',\n",
    "    'Slovakia': 'SK', 'Slovenia': 'SI', 'Spain': 'ES', 'Sweden': 'SE',\n",
    "    'Switzerland': 'CH',\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"  Simulations: {N_SIM}\")\n",
    "print(f\"  Years: {YEARS[0]}-{YEARS[-1]}\")\n",
    "print(f\"  Adoption scenario: {ADOPTION_SCENARIO}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Country map entries: {len(COUNTRY_NAME_TO_CODE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece4f44",
   "metadata": {},
   "source": [
    "## 2. Core Functions (from Task2 Simulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b23739",
   "metadata": {},
   "source": [
    "### 2.1 Calendar Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97612816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calendar test: 365 days in 2027\n",
      "Cyber Week days: 7\n"
     ]
    }
   ],
   "source": [
    "def build_year_calendar(year: int) -> pd.DataFrame:\n",
    "    \"\"\"Build calendar with Cyber Week and period assignments.\"\"\"\n",
    "    start = date(year, 1, 1)\n",
    "    end = date(year, 12, 31)\n",
    "    dates = pd.date_range(start, end, freq='D')\n",
    "    \n",
    "    cal = pd.DataFrame({'date': dates})\n",
    "    cal['day_of_week'] = cal['date'].dt.dayofweek\n",
    "    cal['week'] = cal['date'].dt.isocalendar().week\n",
    "    \n",
    "    # Cyber Week: week containing Black Friday (4th Thursday of November)\n",
    "    nov_days = [d for d in dates if d.month == 11]\n",
    "    thursdays = [d for d in nov_days if d.dayofweek == 3]\n",
    "    if len(thursdays) >= 4:\n",
    "        bf = thursdays[3]\n",
    "        cyber_week = bf.isocalendar()[1]\n",
    "        cal['is_cyber_week'] = cal['week'] == cyber_week\n",
    "    else:\n",
    "        cal['is_cyber_week'] = False\n",
    "    \n",
    "    # Period assignment (1-13)\n",
    "    cal['period'] = ((cal['date'].dt.dayofyear - 1) // 28) + 1\n",
    "    cal.loc[cal['period'] > 13, 'period'] = 13\n",
    "    \n",
    "    return cal\n",
    "\n",
    "# Test\n",
    "test_cal = build_year_calendar(2027)\n",
    "print(f\"Calendar test: {len(test_cal)} days in 2027\")\n",
    "print(f\"Cyber Week days: {test_cal['is_cyber_week'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac084dc3",
   "metadata": {},
   "source": [
    "### 2.2 Adoption Rate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6ba612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market year 1: 0.0020\n",
      "Market year 3: 0.0050\n",
      "Market year 5: 0.0120\n",
      "Market year 8: 0.0320\n"
     ]
    }
   ],
   "source": [
    "def adoption_rate_by_scenario(market_year: int, scenario: str = 'mp') -> float:\n",
    "    \"\"\"\n",
    "    Returns adoption rate for a given market year under specified scenario.\n",
    "    market_year: 1-based (year 1 = entry year)\n",
    "    scenario: 'pes' (pessimistic), 'mp' (most probable), 'opt' (optimistic)\n",
    "    \"\"\"\n",
    "    scenario = scenario.lower().strip()\n",
    "    \n",
    "    rates = {\n",
    "        'pes': [0.001, 0.0015, 0.002, 0.003, 0.004, 0.006, 0.008, 0.010],\n",
    "        'mp':  [0.002, 0.003, 0.005, 0.008, 0.012, 0.018, 0.025, 0.032],\n",
    "        'opt': [0.003, 0.006, 0.012, 0.020, 0.032, 0.048, 0.064, 0.080],\n",
    "    }\n",
    "    \n",
    "    if scenario not in rates:\n",
    "        raise ValueError(f\"Invalid scenario: {scenario}\")\n",
    "    \n",
    "    idx = min(market_year - 1, len(rates[scenario]) - 1)\n",
    "    return rates[scenario][max(0, idx)]\n",
    "\n",
    "# Test\n",
    "for yr in [1, 3, 5, 8]:\n",
    "    print(f\"Market year {yr}: {adoption_rate_by_scenario(yr, 'mp'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb33dbf",
   "metadata": {},
   "source": [
    "### 2.3 Period Shares (Triangular Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e11a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period shares shape: (3, 13)\n",
      "Sample shares (sim 0): [0.0834 0.0694 0.0884 0.0796 0.0514 0.0996 0.0827 0.084  0.054  0.0698\n",
      " 0.0667 0.0938 0.0772]\n",
      "Sum: 1.000000\n"
     ]
    }
   ],
   "source": [
    "def simulate_period_shares(n_sim: int = 1, seed: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate period shares (13 periods) using triangular distribution.\n",
    "    Returns: array of shape (n_sim, 13) with shares summing to 1.0\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    draws = rng.triangular(left=0.5, mode=1.0, right=1.5, size=(n_sim, 13))\n",
    "    return draws / draws.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Test\n",
    "test_shares = simulate_period_shares(n_sim=3, seed=42)\n",
    "print(f\"Period shares shape: {test_shares.shape}\")\n",
    "print(f\"Sample shares (sim 0): {test_shares[0].round(4)}\")\n",
    "print(f\"Sum: {test_shares[0].sum():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f82b5c",
   "metadata": {},
   "source": [
    "### 2.4 Model Shares (Triangular Variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4db046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model shares function loaded\n"
     ]
    }
   ],
   "source": [
    "def triangular_model_shares(base_shares: np.ndarray, sim: int, year: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply triangular variation to base model shares.\n",
    "    Returns: array of 24 model shares summing to 1.0\n",
    "    \"\"\"\n",
    "    seed_val = sim * 10000 + year\n",
    "    rng = np.random.default_rng(seed_val)\n",
    "    draws = rng.triangular(left=0.8, mode=1.0, right=1.2, size=len(base_shares))\n",
    "    adjusted = base_shares * draws\n",
    "    return adjusted / adjusted.sum()\n",
    "\n",
    "# Test (will test after loading model_df)\n",
    "print(\"Model shares function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7855b",
   "metadata": {},
   "source": [
    "### 2.5 OTD Conversion Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25937bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metro, 0.5 days: 1.00\n",
      "Metro, 2.5 days: 0.85\n",
      "Non-Metro, 3.5 days: 0.85\n"
     ]
    }
   ],
   "source": [
    "def get_otd_conversion_rate(segment: str, otd_days: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate purchase probability based on OTD bucket.\n",
    "    segment: 'Metro' or 'Non-Metro'\n",
    "    otd_days: order-to-delivery time in days\n",
    "    \"\"\"\n",
    "    # Bucket OTD\n",
    "    if otd_days <= 1.0:\n",
    "        bucket = 0\n",
    "    elif otd_days <= 2.0:\n",
    "        bucket = 1\n",
    "    elif otd_days <= 3.0:\n",
    "        bucket = 2\n",
    "    else:\n",
    "        bucket = 3\n",
    "    \n",
    "    # Conversion rates by bucket and segment\n",
    "    conversion_matrix = {\n",
    "        'Metro': [1.00, 0.95, 0.85, 0.70],\n",
    "        'Non-Metro': [1.00, 0.98, 0.93, 0.85],\n",
    "    }\n",
    "    \n",
    "    return conversion_matrix.get(segment, [0.7]*4)[bucket]\n",
    "\n",
    "# Test\n",
    "print(f\"Metro, 0.5 days: {get_otd_conversion_rate('Metro', 0.5):.2f}\")\n",
    "print(f\"Metro, 2.5 days: {get_otd_conversion_rate('Metro', 2.5):.2f}\")\n",
    "print(f\"Non-Metro, 3.5 days: {get_otd_conversion_rate('Non-Metro', 3.5):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6da075",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ea0db",
   "metadata": {},
   "source": [
    "### 3.1 Load Population and Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc983518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ../Task4/Pop_inputs.xlsx\n",
      "  Metro cities: 273\n",
      "  Non-metro countries: 29\n",
      "  Product models: 24\n",
      "\n",
      "Model categories: {'Floor Care': 4, 'Kitchen Help': 4, 'Safety & Security': 4, 'Wall & Window': 4, 'Leisure': 4, 'Exterior Care': 4}\n",
      "Price range: €360 - €720\n",
      "\n",
      "Metro country_code sample:\n",
      "Country country_code      City  city_key\n",
      "Belgium           BE  Brussels  brussels\n",
      "Belgium           BE   Antwerp   antwerp\n",
      "Belgium           BE     Liege     liege\n",
      "Belgium           BE      Gent      gent\n",
      "Belgium           BE Charleroi charleroi\n"
     ]
    }
   ],
   "source": [
    "# Load BotWorld inputs\n",
    "input_file = TASK4_DIR / 'Pop_inputs.xlsx'\n",
    "\n",
    "print(f\"Loading data from: {input_file}\")\n",
    "\n",
    "# Metro cities with population projections\n",
    "metro_df = pd.read_excel(input_file, sheet_name='Metro')\n",
    "print(f\"  Metro cities: {len(metro_df)}\")\n",
    "\n",
    "# Non-metro population by country\n",
    "nonmetro_df = pd.read_excel(input_file, sheet_name='NonMetro')\n",
    "print(f\"  Non-metro countries: {len(nonmetro_df)}\")\n",
    "\n",
    "# 24 product models with prices and market shares\n",
    "model_df = pd.read_excel(input_file, sheet_name='Model')\n",
    "print(f\"  Product models: {len(model_df)}\")\n",
    "\n",
    "# ── Add country_code column (ISO 2-letter) to align with ENTRY_YEAR_MAP & assignment_df ──\n",
    "# Pop_inputs.xlsx uses full country names; all lookups use ISO codes\n",
    "metro_df['country_code'] = metro_df['Country'].map(COUNTRY_NAME_TO_CODE)\n",
    "nonmetro_df['country_code'] = nonmetro_df['Country'].map(COUNTRY_NAME_TO_CODE)\n",
    "\n",
    "unmapped_metro = metro_df[metro_df['country_code'].isna()]['Country'].unique()\n",
    "unmapped_nm = nonmetro_df[nonmetro_df['country_code'].isna()]['Country'].unique()\n",
    "if len(unmapped_metro) > 0:\n",
    "    print(f\"  WARNING - Metro countries not in COUNTRY_NAME_TO_CODE: {unmapped_metro}\")\n",
    "if len(unmapped_nm) > 0:\n",
    "    print(f\"  WARNING - NonMetro countries not in COUNTRY_NAME_TO_CODE: {unmapped_nm}\")\n",
    "\n",
    "# Normalize city name for OTD lookup: lowercase, spaces → underscores\n",
    "# (assignment_df node_ids use this format: e.g. 'frankfurt_am_main')\n",
    "metro_df['city_key'] = metro_df['City'].str.lower().str.replace(' ', '_', regex=False)\n",
    "\n",
    "# Prepare model data\n",
    "model_df['Share_MP'] = model_df['Share_MP'] / model_df['Share_MP'].sum()\n",
    "model_codes = model_df['Model'].values\n",
    "model_prices = model_df['Price_EUR'].values\n",
    "model_shares_base = model_df['Share_MP'].values\n",
    "\n",
    "print(f\"\\nModel categories: {model_df['Category'].value_counts().to_dict()}\")\n",
    "print(f\"Price range: €{model_prices.min():.0f} - €{model_prices.max():.0f}\")\n",
    "print(f\"\\nMetro country_code sample:\")\n",
    "print(metro_df[['Country', 'country_code', 'City', 'city_key']].head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3002dd",
   "metadata": {},
   "source": [
    "### 3.2 Load DC Assignment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c3fba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded assignment file: 1966 records\n",
      "  Years: [2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034]\n",
      "  Countries: 30\n",
      "  DCs: 4\n",
      "  Node types: {'metro': 1786, 'non_metro': 180}\n",
      "\n",
      "Sample assignments:\n",
      "   year             node_id  assigned_cand  units country segment\n",
      "0  2027   METRO_BE_brussels  CAND_DE_koeln    540      BE   Metro\n",
      "1  2027    METRO_BE_antwerp  CAND_DE_koeln    268      BE   Metro\n",
      "2  2027      METRO_BE_liege  CAND_DE_koeln    173      BE   Metro\n",
      "3  2027       METRO_BE_gent  CAND_DE_koeln    121      BE   Metro\n",
      "4  2027  METRO_BE_charleroi  CAND_DE_koeln    105      BE   Metro\n"
     ]
    }
   ],
   "source": [
    "# Load city-to-DC assignment from Task2\n",
    "assignment_file = TASK2_DIR / 'assignment_with_otd_prob_reachable.csv'\n",
    "assignment_df = pd.read_csv(assignment_file)\n",
    "\n",
    "print(f\"Loaded assignment file: {len(assignment_df)} records\")\n",
    "\n",
    "# Extract country and segment from node_id\n",
    "assignment_df['country'] = assignment_df['node_id'].str.extract(r'_(..?)_')[0]\n",
    "assignment_df['segment'] = assignment_df['node_type'].apply(\n",
    "    lambda x: 'Metro' if x == 'metro' else 'Non-Metro'\n",
    ")\n",
    "\n",
    "print(f\"  Years: {sorted(assignment_df['year'].unique())}\")\n",
    "print(f\"  Countries: {len(assignment_df['country'].unique())}\")\n",
    "print(f\"  DCs: {len(assignment_df['assigned_cand'].unique())}\")\n",
    "print(f\"  Node types: {assignment_df['node_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# Sample\n",
    "print(\"\\nSample assignments:\")\n",
    "print(assignment_df[['year', 'node_id', 'assigned_cand', 'units', 'country', 'segment']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a82cf2",
   "metadata": {},
   "source": [
    "### 3.3 Prepare City Proportion Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b56c156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City proportions calculated: 1966 entries\n",
      "  Proportion sums - Min: 1.000000, Max: 1.000000\n",
      "\n",
      "Sample proportions (Germany Metro 2027):\n",
      "                   node_id  proportion assigned_cand\n",
      "           METRO_DE_berlin    0.143842 CAND_DE_koeln\n",
      "          METRO_DE_hamburg    0.071841 CAND_DE_koeln\n",
      "           METRO_DE_munich    0.064144 CAND_DE_koeln\n",
      "            METRO_DE_koeln    0.038807 CAND_DE_koeln\n",
      "METRO_DE_frankfurt_am_main    0.026620 CAND_DE_koeln\n",
      "      METRO_DE_duesseldorf    0.025818 CAND_DE_koeln\n",
      "        METRO_DE_stuttgart    0.025657 CAND_DE_koeln\n",
      "          METRO_DE_leipzig    0.024535 CAND_DE_koeln\n",
      "          METRO_DE_dresden    0.024054 CAND_DE_koeln\n",
      "         METRO_DE_dortmund    0.023733 CAND_DE_koeln\n"
     ]
    }
   ],
   "source": [
    "# Calculate city proportions within each (year, country, segment)\n",
    "# This will be used to disaggregate segment-level demand to city-level\n",
    "\n",
    "def calculate_city_proportions(assignment_df):\n",
    "    \"\"\"\n",
    "    For each (year, country, segment), calculate what proportion each city represents.\n",
    "    Returns: DataFrame with columns [year, country, segment, node_id, proportion, assigned_cand]\n",
    "    \"\"\"\n",
    "    # Calculate segment totals\n",
    "    segment_totals = assignment_df.groupby(['year', 'country', 'segment'])['units'].transform('sum')\n",
    "    \n",
    "    # Calculate proportions\n",
    "    proportions = assignment_df.copy()\n",
    "    proportions['proportion'] = proportions['units'] / segment_totals\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    proportions = proportions[['year', 'country', 'segment', 'node_id', 'proportion', 'assigned_cand', 'units']]\n",
    "    \n",
    "    return proportions\n",
    "\n",
    "city_proportions = calculate_city_proportions(assignment_df)\n",
    "\n",
    "print(f\"City proportions calculated: {len(city_proportions)} entries\")\n",
    "\n",
    "# Validation: proportions should sum to 1.0 within each segment\n",
    "validation = city_proportions.groupby(['year', 'country', 'segment'])['proportion'].sum()\n",
    "print(f\"  Proportion sums - Min: {validation.min():.6f}, Max: {validation.max():.6f}\")\n",
    "\n",
    "# Sample\n",
    "print(\"\\nSample proportions (Germany Metro 2027):\")\n",
    "sample = city_proportions[\n",
    "    (city_proportions['year'] == 2027) & \n",
    "    (city_proportions['country'] == 'DE') & \n",
    "    (city_proportions['segment'] == 'Metro')\n",
    "].head(10)\n",
    "print(sample[['node_id', 'proportion', 'assigned_cand']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "zzbyi1t1o2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment-to-DC weights pre-computed: 223 rows\n",
      "  (bypasses 1966-row city-level intermediate)\n",
      "\n",
      "Weight-sum check — Min: 1.000000  Max: 1.000000\n",
      "\n",
      "DCs per segment group:\n",
      "euro_dc_id\n",
      "1    143\n",
      "2     31\n",
      "3      6\n",
      "\n",
      "Sample (DE Metro 2027):\n",
      " year country segment    euro_dc_id  dc_weight\n",
      " 2027      DE   Metro CAND_DE_koeln        1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# 3.3b  Pre-aggregate city proportions → segment-to-DC weights\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# WHY: disaggregate_to_city_level() merges 27M segment rows with 1966 city rows\n",
    "#      → ~271M intermediate rows (~63 GB peak with pandas overhead) → OOM.\n",
    "#\n",
    "# FIX: collapse city_proportions to segment_dc_weights FIRST.\n",
    "#      For each (year, country, segment, dc): weight = sum of population\n",
    "#      proportions for all cities in that segment that are assigned to that DC.\n",
    "#\n",
    "#      Result: at most 4 rows per segment group (one per candidate DC) instead\n",
    "#      of ~10 city rows.  Merge expansion: 27M × ~1.1 ≈ 30M — stays < 3 GB.\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "segment_dc_weights = (\n",
    "    city_proportions\n",
    "    .groupby(['year', 'country', 'segment', 'assigned_cand'], as_index=False)['proportion']\n",
    "    .sum()\n",
    "    .rename(columns={'assigned_cand': 'euro_dc_id', 'proportion': 'dc_weight'})\n",
    ")\n",
    "\n",
    "print(f\"Segment-to-DC weights pre-computed: {len(segment_dc_weights)} rows\")\n",
    "print(f\"  (bypasses {len(city_proportions)}-row city-level intermediate)\")\n",
    "\n",
    "# Validate: dc_weights must sum to 1.0 per (year, country, segment)\n",
    "wt_check = segment_dc_weights.groupby(['year', 'country', 'segment'])['dc_weight'].sum()\n",
    "print(f\"\\nWeight-sum check — Min: {wt_check.min():.6f}  Max: {wt_check.max():.6f}\")\n",
    "\n",
    "# Show how many DCs serve each segment group (should almost always be 1)\n",
    "dcs_per_group = segment_dc_weights.groupby(['year', 'country', 'segment'])['euro_dc_id'].count()\n",
    "print(f\"\\nDCs per segment group:\")\n",
    "print(dcs_per_group.value_counts().sort_index().to_string())\n",
    "\n",
    "print(f\"\\nSample (DE Metro 2027):\")\n",
    "print(\n",
    "    segment_dc_weights[\n",
    "        (segment_dc_weights['year'] == 2027) &\n",
    "        (segment_dc_weights['country'] == 'DE') &\n",
    "        (segment_dc_weights['segment'] == 'Metro')\n",
    "    ].to_string(index=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67777e",
   "metadata": {},
   "source": [
    "### 3.4 Load OTD Data (from Task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a70518b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTD data loaded:\n",
      "  Metro cities with OTD: 272\n",
      "  Non-metro countries with OTD: 29\n",
      "  Non-metro countries: ['AT', 'BE', 'BG', 'CH', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GR', 'HR', 'HU', 'IE', 'IT', 'LT', 'LU', 'LV', 'MT', 'NL', 'NO', 'PL', 'PT', 'RO', 'SE', 'SI', 'SK']\n",
      "  Country-level metro OTD averages: 180 entries\n",
      "\n",
      "Sample metro_city_otd keys: ['brussels', 'antwerp', 'liege', 'gent', 'charleroi']\n",
      "Sample nonmetro_otd: {'BE': {2027: 1, 2028: 1, 2029: 1, 2030: 1, 2031: 1, 2032: 1, 2033: 1, 2034: 1}, 'DE': {2027: 2, 2028: 2, 2029: 2, 2030: 2, 2031: 2, 2032: 2, 2033: 2, 2034: 2}, 'LU': {2027: 1, 2028: 1, 2029: 1, 2030: 1, 2031: 1, 2032: 1, 2033: 1, 2034: 1}}\n"
     ]
    }
   ],
   "source": [
    "# Load OTD data from assignment file\n",
    "# node_id formats:\n",
    "#   Metro:    METRO_BE_brussels       → country = 'BE', city_key = 'brussels'\n",
    "#   NonMetro: NONMETRO_BE             → country = 'BE'  (no trailing city segment)\n",
    "# The original regex r'_(..?)_' fails for NONMETRO_BE (no second underscore)\n",
    "# Fix: extract second token by splitting on '_'\n",
    "\n",
    "metro_city_otd = {}     # {city_key: {year: otd_days}}   city_key = lowercase underscore\n",
    "nonmetro_otd = {}       # {country_code: {year: otd_days}}\n",
    "\n",
    "for _, row in assignment_df.iterrows():\n",
    "    year = row['year']\n",
    "    node_id = row['node_id']\n",
    "    otd_days = row['otd_days_promise']\n",
    "    tokens = node_id.split('_')   # e.g. ['METRO','BE','brussels'] or ['NONMETRO','BE']\n",
    "    country_code = tokens[1]      # always the second token\n",
    "\n",
    "    if row['node_type'] == 'metro':\n",
    "        # city_key = everything after 'METRO_CC_'\n",
    "        city_key = '_'.join(tokens[2:])   # handles multi-word cities like 'frankfurt_am_main'\n",
    "        if city_key not in metro_city_otd:\n",
    "            metro_city_otd[city_key] = {}\n",
    "        metro_city_otd[city_key][year] = otd_days\n",
    "    else:\n",
    "        if country_code not in nonmetro_otd:\n",
    "            nonmetro_otd[country_code] = {}\n",
    "        nonmetro_otd[country_code][year] = otd_days\n",
    "\n",
    "print(f\"OTD data loaded:\")\n",
    "print(f\"  Metro cities with OTD: {len(metro_city_otd)}\")\n",
    "print(f\"  Non-metro countries with OTD: {len(nonmetro_otd)}\")\n",
    "print(f\"  Non-metro countries: {sorted(nonmetro_otd.keys())}\")\n",
    "\n",
    "# For country-level population-weighted metro OTD averages (used as fallback)\n",
    "country_metro_otd_avg = (\n",
    "    assignment_df[assignment_df['node_type'] == 'metro']\n",
    "    .groupby(['year', 'country'])\n",
    "    .apply(\n",
    "        lambda x: (x['otd_days_promise'] * x['units']).sum() / x['units'].sum()\n",
    "        if x['units'].sum() > 0 else 2.0,\n",
    "        include_groups=False\n",
    "    )\n",
    "    .to_dict()\n",
    ")\n",
    "print(f\"  Country-level metro OTD averages: {len(country_metro_otd_avg)} entries\")\n",
    "\n",
    "# Spot check\n",
    "print(f\"\\nSample metro_city_otd keys: {list(metro_city_otd.keys())[:5]}\")\n",
    "print(f\"Sample nonmetro_otd: {dict(list(nonmetro_otd.items())[:3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480fce72",
   "metadata": {},
   "source": [
    "## 4. Modified Simulator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcfd07b",
   "metadata": {},
   "source": [
    "**Key Modification**: Instead of aggregating to `(sim, date, model)`, \n",
    "we save `(sim, date, year, country, segment, model, sales_units)` \n",
    "to enable city-level disaggregation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5382008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_modified_simulator(\n",
    "    metro_df, nonmetro_df, model_df, entry_year_map, years,\n",
    "    metro_city_otd, nonmetro_otd, country_metro_otd_avg,\n",
    "    n_sim=100, seed=42, batch_size=10, out_dir=None, adoption_scenario='mp'\n",
    "):\n",
    "    \"\"\"\n",
    "    Modified OTD simulator that saves (country_code, segment) information.\n",
    "    \n",
    "    Key fixes vs original:\n",
    "    - Uses metro_df['country_code'] (ISO 2-letter) to match ENTRY_YEAR_MAP and city_proportions\n",
    "    - Uses metro_df['city_key'] (lowercase_underscore) to match metro_city_otd keys\n",
    "    - Uses nonmetro_otd with ISO country codes (fixed extraction in cell-23)\n",
    "    \n",
    "    Output per batch: (sim, date, year, country, segment, model, sales_units)\n",
    "    where 'country' is ISO 2-letter code to align with city_proportions merge.\n",
    "    \"\"\"\n",
    "    adoption_scenario = adoption_scenario.lower().strip()\n",
    "    if adoption_scenario not in {'pes', 'mp', 'opt'}:\n",
    "        raise ValueError(f\"adoption_scenario must be one of {{'pes','mp','opt'}}, got '{adoption_scenario}'\")\n",
    "    \n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    rng_global = np.random.default_rng(seed)\n",
    "    model_shares_base = model_df['Share_MP'].values\n",
    "    model_codes = model_df['Model'].values\n",
    "    model_prices = model_df['Price_EUR'].values\n",
    "    \n",
    "    sim_records = []\n",
    "    annual_summary = []\n",
    "    segment_summary = []\n",
    "    \n",
    "    for sim in range(n_sim):\n",
    "        print(f'  Sim {sim+1}/{n_sim}...', end=' ', flush=True)\n",
    "        \n",
    "        for yr in years:\n",
    "            cal = build_year_calendar(yr)\n",
    "            cw_days = cal[cal['is_cyber_week']]['date'].values\n",
    "            ncw_days = cal[~cal['is_cyber_week']]\n",
    "            n_cw = len(cw_days)\n",
    "            \n",
    "            # ── 1. Annual demand by country_code-segment ──\n",
    "            segment_annual = {}\n",
    "            \n",
    "            for _, city_row in metro_df.iterrows():\n",
    "                country = city_row['country_code']   # ISO 2-letter code\n",
    "                if pd.isna(country) or country not in entry_year_map:\n",
    "                    continue\n",
    "                market_year = yr - entry_year_map[country] + 1\n",
    "                if market_year < 1:\n",
    "                    continue\n",
    "                pop = city_row[f'Pop_{yr}']\n",
    "                rate = adoption_rate_by_scenario(market_year, adoption_scenario)\n",
    "                key = (country, 'Metro')\n",
    "                segment_annual[key] = segment_annual.get(key, 0) + pop * rate\n",
    "            \n",
    "            for _, nm_row in nonmetro_df.iterrows():\n",
    "                country = nm_row['country_code']     # ISO 2-letter code\n",
    "                if pd.isna(country) or country not in entry_year_map:\n",
    "                    continue\n",
    "                market_year = yr - entry_year_map[country] + 1\n",
    "                if market_year < 1:\n",
    "                    continue\n",
    "                pop = nm_row[f'NonMetroPop_{yr}']\n",
    "                rate = adoption_rate_by_scenario(market_year, adoption_scenario)\n",
    "                segment_annual[(country, 'Non-Metro')] = pop * rate\n",
    "            \n",
    "            if not segment_annual:\n",
    "                continue\n",
    "            total_annual = sum(segment_annual.values())\n",
    "            if total_annual == 0:\n",
    "                continue\n",
    "            \n",
    "            # ── 2. Period shares ──\n",
    "            period_shares = simulate_period_shares(\n",
    "                n_sim=1, seed=int(rng_global.integers(0, 1_000_000))\n",
    "            )[0]\n",
    "            \n",
    "            # ── 3. Cyber vs regular split ──\n",
    "            cyber_total = total_annual * CYBER_WEEK_SHARE\n",
    "            regular_total = total_annual * (1.0 - CYBER_WEEK_SHARE)\n",
    "            \n",
    "            # ── 4. Daily demand allocation ──\n",
    "            day_demand_total = {}\n",
    "            for p in range(1, 14):\n",
    "                period_units = regular_total * period_shares[p - 1]\n",
    "                p_days = ncw_days[ncw_days['period'] == p]\n",
    "                if len(p_days) == 0:\n",
    "                    continue\n",
    "                dow_w = p_days['day_of_week'].map(lambda d: DOW_WEIGHTS[d]).values\n",
    "                dow_w = dow_w / dow_w.sum()\n",
    "                for d_idx, (_, day_row) in enumerate(p_days.iterrows()):\n",
    "                    day_demand_total[day_row['date']] = period_units * dow_w[d_idx]\n",
    "            \n",
    "            if n_cw > 0:\n",
    "                cw_cal = cal[cal['is_cyber_week']]\n",
    "                cw_dow_w = cw_cal['day_of_week'].map(lambda d: DOW_WEIGHTS[d]).values\n",
    "                cw_dow_w = cw_dow_w / cw_dow_w.sum()\n",
    "                for i, (_, cw_row) in enumerate(cw_cal.iterrows()):\n",
    "                    day_demand_total[cw_row['date']] = cyber_total * cw_dow_w[i]\n",
    "            \n",
    "            # ── 5. Segment weights & OTD ──\n",
    "            segment_weights = {k: v / total_annual for k, v in segment_annual.items()}\n",
    "            cw_date_set = {pd.Timestamp(x).date() if not isinstance(x, date) else x\n",
    "                          for x in cw_days}\n",
    "            \n",
    "            # Country-level population-weighted metro OTD for the year\n",
    "            # Uses city_key (lowercase_underscore) to match metro_city_otd keys\n",
    "            country_metro_otd_yr = {}\n",
    "            for _, city_row in metro_df.iterrows():\n",
    "                country = city_row['country_code']\n",
    "                if pd.isna(country):\n",
    "                    continue\n",
    "                city_key = city_row['city_key']       # normalized: 'frankfurt_am_main'\n",
    "                pop = city_row[f'Pop_{yr}']\n",
    "                city_otd = metro_city_otd.get(city_key, {}).get(yr, 2.0)\n",
    "                if country not in country_metro_otd_yr:\n",
    "                    country_metro_otd_yr[country] = {'weighted_sum': 0.0, 'total_pop': 0.0}\n",
    "                country_metro_otd_yr[country]['weighted_sum'] += city_otd * pop\n",
    "                country_metro_otd_yr[country]['total_pop'] += pop\n",
    "            \n",
    "            for country in country_metro_otd_yr:\n",
    "                tp = country_metro_otd_yr[country]['total_pop']\n",
    "                country_metro_otd_yr[country] = (\n",
    "                    country_metro_otd_yr[country]['weighted_sum'] / tp if tp > 0 else 2.0\n",
    "                )\n",
    "            \n",
    "            # ── 6. OTD conversion at segment level ──\n",
    "            daily_segment_sales = {}\n",
    "            for d, total_units in day_demand_total.items():\n",
    "                is_cw = d in cw_date_set\n",
    "                for (country, segment), seg_weight in segment_weights.items():\n",
    "                    seg_demand = total_units * seg_weight\n",
    "                    if segment == 'Metro':\n",
    "                        otd_days = country_metro_otd_yr.get(country, 2.0)\n",
    "                    else:\n",
    "                        otd_days = nonmetro_otd.get(country, {}).get(yr, 5.0)\n",
    "                    conversion_rate = get_otd_conversion_rate(segment, otd_days)\n",
    "                    daily_segment_sales[(d, country, segment, is_cw)] = {\n",
    "                        'demand': seg_demand,\n",
    "                        'sales': seg_demand * conversion_rate,\n",
    "                        'otd_days': otd_days,\n",
    "                        'conversion_rate': conversion_rate,\n",
    "                    }\n",
    "            \n",
    "            # ── 7. Decompose to 24 products ──\n",
    "            model_shares = triangular_model_shares(model_shares_base, sim, yr)\n",
    "            for (d, country, segment, is_cw), agg in daily_segment_sales.items():\n",
    "                ms_vec = agg['sales'] * model_shares\n",
    "                md_vec = agg['demand'] * model_shares\n",
    "                p_factor = (1.0 - CYBER_PRICE_DISCOUNT) if is_cw else 1.0\n",
    "                for m_idx, mdl in enumerate(model_codes):\n",
    "                    ms = ms_vec[m_idx]\n",
    "                    if ms < 1e-9:\n",
    "                        continue\n",
    "                    sim_records.append({\n",
    "                        'sim': sim,\n",
    "                        'date': pd.Timestamp(d),\n",
    "                        'year': yr,\n",
    "                        'country': country,       # ISO 2-letter code\n",
    "                        'segment': segment,\n",
    "                        'is_cyber_week': is_cw,\n",
    "                        'demand_units': md_vec[m_idx],\n",
    "                        'otd_days': agg['otd_days'],\n",
    "                        'conversion_rate': agg['conversion_rate'],\n",
    "                        'sales_units': ms,\n",
    "                        'model': mdl,\n",
    "                        'revenue': ms * model_prices[m_idx] * p_factor,\n",
    "                    })\n",
    "        \n",
    "        print('Done')\n",
    "        \n",
    "        # ── Batch flush with country & segment preserved ──\n",
    "        if batch_size and out_dir and (sim + 1) % batch_size == 0:\n",
    "            batch_idx = (sim + 1) // batch_size - 1\n",
    "            batch_df = pd.DataFrame(sim_records)\n",
    "            \n",
    "            # Save (sim, date, year, country, segment, model, sales_units)\n",
    "            # country is ISO 2-letter code → aligns with city_proportions merge\n",
    "            segment_batch = (\n",
    "                batch_df[['sim', 'date', 'year', 'country', 'segment', 'model', 'sales_units']]\n",
    "                .groupby(['sim', 'date', 'year', 'country', 'segment', 'model'], as_index=False)\n",
    "                .agg(sales_units=('sales_units', 'sum'))\n",
    "            )\n",
    "            \n",
    "            out_path = os.path.join(out_dir, f'batch_{batch_idx:02d}.csv.gz')\n",
    "            segment_batch.to_csv(out_path, index=False, compression='gzip')\n",
    "            print(f'    → Saved batch {batch_idx} to {out_path}')\n",
    "            \n",
    "            # Annual summary\n",
    "            annual_summary.append(\n",
    "                batch_df.groupby(['sim', 'year']).agg(\n",
    "                    demand_units=('demand_units', 'sum'),\n",
    "                    sales_units=('sales_units', 'sum'),\n",
    "                    revenue=('revenue', 'sum'),\n",
    "                ).reset_index()\n",
    "            )\n",
    "            \n",
    "            # Segment summary\n",
    "            segment_summary.append(\n",
    "                batch_df.groupby(['sim', 'year', 'country', 'segment']).agg(\n",
    "                    demand_units=('demand_units', 'sum'),\n",
    "                    sales_units=('sales_units', 'sum'),\n",
    "                    otd_days=('otd_days', 'mean'),\n",
    "                ).reset_index()\n",
    "            )\n",
    "            \n",
    "            del batch_df, segment_batch\n",
    "            sim_records = []\n",
    "            gc.collect()\n",
    "    \n",
    "    if annual_summary:\n",
    "        annual_df = pd.concat(annual_summary, ignore_index=True)\n",
    "        segment_df = pd.concat(segment_summary, ignore_index=True)\n",
    "        return {'annual': annual_df, 'segment': segment_df}\n",
    "    else:\n",
    "        return pd.DataFrame(sim_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c692003",
   "metadata": {},
   "source": [
    "## 5. City-Level Disaggregation & DC Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d337c1",
   "metadata": {},
   "source": [
    "### 5.1 Disaggregate Segment-Level to City-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c21ae7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disaggregate_to_city_level(segment_df, city_proportions):\n",
    "    \"\"\"\n",
    "    Disaggregate segment-level demand to city-level using population proportions.\n",
    "    \n",
    "    Input: (sim, date, year, country, segment, model, sales_units)\n",
    "    Output: (sim, date, year, node_id, euro_dc_id, model, city_demand)\n",
    "    \"\"\"\n",
    "    print(\"\\nDisaggregating segment-level demand to city-level...\")\n",
    "    \n",
    "    # Merge with city proportions\n",
    "    merged = segment_df.merge(\n",
    "        city_proportions,\n",
    "        on=['year', 'country', 'segment'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Calculate city-level demand\n",
    "    merged['city_demand'] = merged['sales_units'] * merged['proportion']\n",
    "    \n",
    "    # Select final columns\n",
    "    city_level = merged[[\n",
    "        'sim', 'date', 'year', 'node_id', 'assigned_cand', 'model', 'city_demand'\n",
    "    ]].copy()\n",
    "    \n",
    "    city_level.rename(columns={'assigned_cand': 'euro_dc_id'}, inplace=True)\n",
    "    \n",
    "    print(f\"  Input records: {len(segment_df):,}\")\n",
    "    print(f\"  Output records: {len(city_level):,}\")\n",
    "    print(f\"  Expansion factor: {len(city_level)/len(segment_df):.1f}x\")\n",
    "    \n",
    "    # Validation: total demand should be conserved\n",
    "    total_in = segment_df['sales_units'].sum()\n",
    "    total_out = city_level['city_demand'].sum()\n",
    "    print(f\"  Demand conservation: {total_out/total_in*100:.2f}%\")\n",
    "    \n",
    "    return city_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97630eb1",
   "metadata": {},
   "source": [
    "### 5.2 Aggregate City-Level to DC-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "824de20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_dc_level(city_df):\n",
    "    \"\"\"\n",
    "    Aggregate city-level demand to DC-level.\n",
    "    \n",
    "    Input: (sim, date, year, node_id, euro_dc_id, model, city_demand)\n",
    "    Output: (sim, date, year, euro_dc_id, model, realized_units)\n",
    "    \"\"\"\n",
    "    print(\"\\nAggregating city-level demand to DC-level...\")\n",
    "    \n",
    "    dc_level = city_df.groupby(\n",
    "        ['sim', 'date', 'year', 'euro_dc_id', 'model'],\n",
    "        as_index=False\n",
    "    ).agg(realized_units=('city_demand', 'sum'))\n",
    "    \n",
    "    print(f\"  Input records: {len(city_df):,}\")\n",
    "    print(f\"  Output records: {len(dc_level):,}\")\n",
    "    print(f\"  Reduction factor: {len(city_df)/len(dc_level):.1f}x\")\n",
    "    \n",
    "    # Validation\n",
    "    total_in = city_df['city_demand'].sum()\n",
    "    total_out = dc_level['realized_units'].sum()\n",
    "    print(f\"  Demand conservation: {total_out/total_in*100:.2f}%\")\n",
    "    \n",
    "    return dc_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6dc69",
   "metadata": {},
   "source": [
    "## 6. Main Execution Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329c596",
   "metadata": {},
   "source": [
    "This section runs the complete pipeline:\n",
    "1. Run modified simulator → Save batches with (country, segment)\n",
    "2. Load all batches\n",
    "3. Disaggregate to city-level\n",
    "4. Aggregate to DC-level\n",
    "5. Save final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ce67476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEPS 2–4: Streaming Batch → Direct DC Aggregation\n",
      "======================================================================\n",
      "Batch files found : 2\n",
      "Segment-to-DC lookup : 223 rows  (~1.2 DCs/group avg)\n",
      "\n",
      "Expected DC output (upper bound): 28,032,000 rows  (1.35 GB in memory)\n",
      "\n",
      "[1/2] batch_01.csv.gz  ...   27,348,000 seg  →  2,279,040 DC rows  |  conservation: 34.94%\n",
      "[2/2] batch_02.csv.gz  ...   27,348,000 seg  →  2,279,040 DC rows  |  conservation: 34.94%\n",
      "\n",
      "All 2 batches processed.\n",
      "Total DC records written  : 4,558,080\n",
      "Overall demand conservation: 34.94%\n",
      "Output : ../Task4/dc_output/dc_daily_demand.csv.gz\n",
      "File size : 42.7 MB\n",
      "\n",
      "Loading dc_output for validation ...\n",
      "dc_output shape: (4558080, 6)\n",
      "Columns: ['sim', 'date', 'year', 'euro_dc_id', 'model', 'realized_units']\n",
      "\n",
      "Sample:\n",
      "   sim        date  year     euro_dc_id model  realized_units\n",
      "0   10  2027-01-01  2027  CAND_DE_koeln   F10       33.552045\n",
      "1   10  2027-01-01  2027  CAND_DE_koeln   F20       17.038681\n",
      "2   10  2027-01-01  2027  CAND_DE_koeln   F30        7.030621\n",
      "3   10  2027-01-01  2027  CAND_DE_koeln   F50        3.831210\n",
      "4   10  2027-01-01  2027  CAND_DE_koeln   K10       22.983636\n",
      "5   10  2027-01-01  2027  CAND_DE_koeln   K20       12.048711\n",
      "6   10  2027-01-01  2027  CAND_DE_koeln   K30        6.066091\n",
      "7   10  2027-01-01  2027  CAND_DE_koeln   K50        3.641838\n",
      "8   10  2027-01-01  2027  CAND_DE_koeln   L10        3.697637\n",
      "9   10  2027-01-01  2027  CAND_DE_koeln   L20        9.496292\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# STEPS 2–4 (combined): Memory-Efficient Streaming Batch → DC Aggregation\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "#\n",
    "# OLD approach  (OOM):\n",
    "#   Load all seg records → city-level merge (27M × ~10 cities = 271M rows, ~63 GB)\n",
    "#                        → aggregate to DC (2.8M rows)\n",
    "#\n",
    "# NEW approach  (memory-safe):\n",
    "#   For each batch one at a time:\n",
    "#     Load seg records (27M) → compact merge with segment_dc_weights (~4 rows/group)\n",
    "#                            → 27M × ~1.1 ≈ 30M rows (<3 GB)\n",
    "#                            → groupby aggregate to DC batch (~2.8M rows)\n",
    "#                            → write to gzip file incrementally\n",
    "#                            → free memory before next batch\n",
    "#\n",
    "# city-level intermediate is NEVER materialised.\n",
    "# Demand is conserved because segment_dc_weights sums to 1.0 per segment group.\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "import gzip\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEPS 2–4: Streaming Batch → Direct DC Aggregation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "batch_files = sorted(OUTPUT_DIR.glob('batch_0[1-2].csv.gz'))\n",
    "print(f\"Batch files found : {len(batch_files)}\")\n",
    "print(f\"Segment-to-DC lookup : {len(segment_dc_weights)} rows  \"\n",
    "      f\"(~{segment_dc_weights.groupby(['year','country','segment'])['euro_dc_id'].count().mean():.1f} DCs/group avg)\")\n",
    "print()\n",
    "\n",
    "output_file = OUTPUT_DIR / 'dc_daily_demand.csv.gz'\n",
    "total_dc_records  = 0\n",
    "total_demand_in   = 0.0\n",
    "total_demand_out  = 0.0\n",
    "\n",
    "# ── Expected final size sanity check ──────────────────────────\n",
    "# 100 sims × 8 years × 365 days × 4 DCs × 24 models = 28,032,000 rows (upper bound)\n",
    "expected_max = N_SIM * len(YEARS) * 365 * 4 * len(model_codes)\n",
    "print(f\"Expected DC output (upper bound): {expected_max:,} rows  \"\n",
    "      f\"({expected_max * 6 * 8 / 1e9:.2f} GB in memory)\")\n",
    "print()\n",
    "\n",
    "with gzip.open(output_file, 'wb') as gz_out:\n",
    "    for i, batch_file in enumerate(batch_files):\n",
    "        print(f\"[{i+1}/{len(batch_files)}] {batch_file.name}\", end=\"  ...  \", flush=True)\n",
    "\n",
    "        # -- Load segment-level batch --\n",
    "        batch_df = pd.read_csv(batch_file)\n",
    "\n",
    "        # -- Compact merge: segment → DC (expansion ≈ 1-4×, not 10×) --\n",
    "        merged = batch_df.merge(\n",
    "            segment_dc_weights,\n",
    "            on=['year', 'country', 'segment'],\n",
    "            how='left'\n",
    "        )\n",
    "        merged['realized_units'] = merged['sales_units'] * merged['dc_weight']\n",
    "\n",
    "        # -- Aggregate to final schema: (sim, date, year, euro_dc_id, model) --\n",
    "        dc_batch = (\n",
    "            merged\n",
    "            .groupby(['sim', 'date', 'year', 'euro_dc_id', 'model'], as_index=False)\n",
    "            .agg(realized_units=('realized_units', 'sum'))\n",
    "        )\n",
    "\n",
    "        demand_in  = batch_df['sales_units'].sum()\n",
    "        demand_out = dc_batch['realized_units'].sum()\n",
    "        total_demand_in  += demand_in\n",
    "        total_demand_out += demand_out\n",
    "        total_dc_records += len(dc_batch)\n",
    "\n",
    "        print(\n",
    "            f\"{len(batch_df):>11,} seg  →  {len(dc_batch):>8,} DC rows  \"\n",
    "            f\"|  conservation: {demand_out / demand_in * 100:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # -- Stream-write (header only on first batch) --\n",
    "        dc_batch.to_csv(gz_out, index=False, header=(i == 0))\n",
    "\n",
    "        # -- Free memory before next batch --\n",
    "        del batch_df, merged, dc_batch\n",
    "        gc.collect()\n",
    "\n",
    "print()\n",
    "print(f\"All {len(batch_files)} batches processed.\")\n",
    "print(f\"Total DC records written  : {total_dc_records:,}\")\n",
    "print(f\"Overall demand conservation: {total_demand_out / total_demand_in * 100:.2f}%\")\n",
    "print(f\"Output : {output_file}\")\n",
    "print(f\"File size : {output_file.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# -- Load back for validation / summary cells below --\n",
    "print(\"\\nLoading dc_output for validation ...\")\n",
    "dc_output = pd.read_csv(output_file)\n",
    "print(f\"dc_output shape: {dc_output.shape}\")\n",
    "print(f\"Columns: {list(dc_output.columns)}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(dc_output.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f17ec22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Final Output Summary\n",
      "======================================================================\n",
      "Output file  : ../Task4/dc_output/dc_daily_demand.csv.gz\n",
      "File size    : 42.7 MB\n",
      "Total records: 4,558,080\n",
      "\n",
      "Schema: (sim, date, year, euro_dc_id, model, realized_units)\n",
      "\n",
      "Size reasonableness:\n",
      "  Sims    : 20\n",
      "  Years   : 8  ([2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034])\n",
      "  DCs     : 4  ['CAND_DE_koeln', 'CAND_ES_madrid', 'CAND_IT_rome', 'CAND_PL_lodz']\n",
      "  Models  : 24\n",
      "  Upper bound (sims×years×365×DCs×models): 5,606,400\n",
      "  Actual / upper bound: 81.3%  (< 100% due to sparse early years)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# STEP 5: Verify Final Output (already written by STEPS 2-4 cell)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Final Output Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"Output file  : {output_file}\")\n",
    "print(f\"File size    : {output_file.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Total records: {len(dc_output):,}\")\n",
    "print(f\"\\nSchema: (sim, date, year, euro_dc_id, model, realized_units)\")\n",
    "\n",
    "# Size reasonableness check\n",
    "n_sims_written   = dc_output['sim'].nunique()\n",
    "n_years_written  = dc_output['year'].nunique()\n",
    "n_dcs_written    = dc_output['euro_dc_id'].nunique()\n",
    "n_models_written = dc_output['model'].nunique()\n",
    "max_possible     = n_sims_written * n_years_written * 365 * n_dcs_written * n_models_written\n",
    "\n",
    "print(f\"\\nSize reasonableness:\")\n",
    "print(f\"  Sims    : {n_sims_written}\")\n",
    "print(f\"  Years   : {n_years_written}  ({sorted(dc_output['year'].unique())})\")\n",
    "print(f\"  DCs     : {n_dcs_written}  {sorted(dc_output['euro_dc_id'].unique())}\")\n",
    "print(f\"  Models  : {n_models_written}\")\n",
    "print(f\"  Upper bound (sims×years×365×DCs×models): {max_possible:,}\")\n",
    "print(f\"  Actual / upper bound: {len(dc_output)/max_possible*100:.1f}%  (< 100% due to sparse early years)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: Saving Final Output\n",
      "======================================================================\n",
      "Saved to: ../Task4/dc_output/dc_daily_demand.csv.gz\n",
      "File size: 42.6 MB\n",
      "Total records: 4,558,080\n",
      "\n",
      "Output schema: (sim, date, year, euro_dc_id, model, realized_units)\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# STEP 5: Save Final Output\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 5: Saving Final Output\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "output_file = OUTPUT_DIR / 'dc_daily_demand.csv.gz'\n",
    "dc_output.to_csv(output_file, index=False, compression='gzip')\n",
    "\n",
    "print(f\"Saved to: {output_file}\")\n",
    "print(f\"File size: {output_file.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Total records: {len(dc_output):,}\")\n",
    "print(\"\\nOutput schema: (sim, date, year, euro_dc_id, model, realized_units)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "1. Total Demand by DC (across all sims)\n",
      "----------------------------------------------------------------------\n",
      "euro_dc_id\n",
      "CAND_DE_koeln     8.375163e+07\n",
      "CAND_PL_lodz      3.961021e+07\n",
      "CAND_IT_rome      3.340370e+07\n",
      "CAND_ES_madrid    3.153517e+07\n",
      "Name: realized_units, dtype: float64\n",
      "\n",
      "Total across all DCs: 188,300,719\n",
      "\n",
      "2. Total Demand by Year (across all sims)\n",
      "----------------------------------------------------------------------\n",
      "year\n",
      "2027     13718.53080\n",
      "2028     24030.42642\n",
      "2029     77442.62050\n",
      "2030    128617.96040\n",
      "2031    203589.94052\n",
      "2032    316951.60573\n",
      "2033    463653.33630\n",
      "2034    655002.76518\n",
      "Name: realized_units, dtype: float64\n",
      "\n",
      "3. Total Demand by Model (top 10)\n",
      "----------------------------------------------------------------------\n",
      "model\n",
      "F10    2.907381e+07\n",
      "K10    2.233068e+07\n",
      "S10    1.704360e+07\n",
      "W10    1.545967e+07\n",
      "F20    1.364777e+07\n",
      "K20    1.027418e+07\n",
      "S20    8.587591e+06\n",
      "L20    8.529884e+06\n",
      "F30    6.947437e+06\n",
      "X20    6.764728e+06\n",
      "Name: realized_units, dtype: float64\n",
      "\n",
      "4. Average Daily Demand by DC (per sim)\n",
      "----------------------------------------------------------------------\n",
      "euro_dc_id\n",
      "CAND_DE_koeln     1433.121709\n",
      "CAND_IT_rome       914.668752\n",
      "CAND_PL_lodz       774.544569\n",
      "CAND_ES_madrid     719.652530\n",
      "Name: realized_units, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate by DC\n",
    "print(\"\\n1. Total Demand by DC (across all sims)\")\n",
    "print(\"-\" * 70)\n",
    "dc_totals = dc_output.groupby('euro_dc_id')['realized_units'].sum().sort_values(ascending=False)\n",
    "print(dc_totals)\n",
    "print(f\"\\nTotal across all DCs: {dc_totals.sum():,.0f}\")\n",
    "\n",
    "# Aggregate by year\n",
    "print(\"\\n2. Total Demand by Year (across all sims)\")\n",
    "print(\"-\" * 70)\n",
    "year_totals = dc_output.groupby('year')['realized_units'].sum() / N_SIM\n",
    "print(year_totals)\n",
    "\n",
    "# Aggregate by model category\n",
    "print(\"\\n3. Total Demand by Model (top 10)\")\n",
    "print(\"-\" * 70)\n",
    "model_totals = dc_output.groupby('model')['realized_units'].sum().sort_values(ascending=False)\n",
    "print(model_totals.head(10))\n",
    "\n",
    "# Average daily demand by DC\n",
    "print(\"\\n4. Average Daily Demand by DC (per sim)\")\n",
    "print(\"-\" * 70)\n",
    "dc_daily_avg = dc_output.groupby(['sim', 'date', 'euro_dc_id'])['realized_units'].sum().groupby('euro_dc_id').mean()\n",
    "print(dc_daily_avg.sort_values(ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "General_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
