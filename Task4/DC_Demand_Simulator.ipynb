{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: DC-Level Daily Demand Simulator\n",
    "\n",
    "**Objective**: Generate DC-level daily realized demand from segment-level simulation\n",
    "\n",
    "**Pipeline**:\n",
    "1. Run modified OTD simulator (saves country, segment info)\n",
    "2. Disaggregate segment demand to city-level using population proportions\n",
    "3. Map cities to DCs using Task2 assignment file\n",
    "4. Aggregate to DC-level daily demand\n",
    "\n",
    "**Output**: `(sim, date, year, euro_dc_id, model, realized_units)`\n",
    "\n",
    "**Configuration**:\n",
    "- Simulations: 100\n",
    "- Years: 2027-2034\n",
    "- Adoption scenario: 'mp' (most probable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c041f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from datetime import date, datetime, timedelta\n",
    "import os\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df886b",
   "metadata": {},
   "source": [
    "## 1. Configuration & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b612e894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "  Simulations: 100\n",
      "  Years: 2027-2034\n",
      "  Adoption scenario: mp\n",
      "  Output directory: ../Task4/dc_output\n",
      "  Country map entries: 29\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "N_SIM = 100\n",
    "SEED = 42\n",
    "YEARS = list(range(2027, 2035))  # 2027-2034\n",
    "ADOPTION_SCENARIO = 'mp'  # most probable\n",
    "BATCH_SIZE = 10  \n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path('../')\n",
    "TASK1_DIR = BASE_DIR / 'Task1'\n",
    "TASK2_DIR = BASE_DIR / 'Task2'\n",
    "TASK4_DIR = BASE_DIR / 'Task4'\n",
    "OUTPUT_DIR = TASK4_DIR / 'dc_output'\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# CONSTANTS (from Task2)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "CYBER_WEEK_SHARE = 0.18\n",
    "CYBER_PRICE_DISCOUNT = 0.15\n",
    "\n",
    "DOW_WEIGHTS = {\n",
    "    0: 0.10,  # Monday\n",
    "    1: 0.12,  # Tuesday\n",
    "    2: 0.13,  # Wednesday\n",
    "    3: 0.14,  # Thursday\n",
    "    4: 0.18,  # Friday\n",
    "    5: 0.19,  # Saturday\n",
    "    6: 0.14,  # Sunday\n",
    "}\n",
    "\n",
    "# Market entry years by country (ISO 2-letter codes)\n",
    "ENTRY_YEAR_MAP = {\n",
    "    'BE': 2027, 'DE': 2027, 'LU': 2027, 'NL': 2027,\n",
    "    'DK': 2028, 'EE': 2028, 'FI': 2028, 'LT': 2028, 'LV': 2028, 'SE': 2028,\n",
    "    'AT': 2029, 'CZ': 2029, 'ES': 2029, 'FR': 2029, 'IT': 2029, 'PL': 2029, 'PT': 2029,\n",
    "    'BG': 2030, 'GR': 2030, 'HR': 2030, 'HU': 2030, 'IE': 2030, 'RO': 2030, 'SI': 2030, 'SK': 2030,\n",
    "}\n",
    "\n",
    "# Full country name → ISO 2-letter code mapping\n",
    "# (Pop_inputs.xlsx uses full names; all other lookups use ISO codes)\n",
    "COUNTRY_NAME_TO_CODE = {\n",
    "    'Austria': 'AT', 'Belgium': 'BE', 'Bulgaria': 'BG', 'Croatia': 'HR',\n",
    "    'Cyprus': 'CY', 'Czech Republic': 'CZ', 'Denmark': 'DK', 'Estonia': 'EE',\n",
    "    'Finland': 'FI', 'France': 'FR', 'Germany': 'DE', 'Greece': 'GR',\n",
    "    'Hungary': 'HU', 'Ireland': 'IE', 'Italy': 'IT', 'Latvia': 'LV',\n",
    "    'Lithuania': 'LT', 'Luxembourg': 'LU', 'Malta': 'MT', 'Netherlands': 'NL',\n",
    "    'Norway': 'NO', 'Poland': 'PL', 'Portugal': 'PT', 'Romania': 'RO',\n",
    "    'Slovakia': 'SK', 'Slovenia': 'SI', 'Spain': 'ES', 'Sweden': 'SE',\n",
    "    'Switzerland': 'CH',\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"  Simulations: {N_SIM}\")\n",
    "print(f\"  Years: {YEARS[0]}-{YEARS[-1]}\")\n",
    "print(f\"  Adoption scenario: {ADOPTION_SCENARIO}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Country map entries: {len(COUNTRY_NAME_TO_CODE)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece4f44",
   "metadata": {},
   "source": [
    "## 2. Core Functions (from Task2 Simulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b23739",
   "metadata": {},
   "source": [
    "### 2.1 Calendar Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97612816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calendar test: 365 days in 2027\n",
      "Cyber Week days: 7\n"
     ]
    }
   ],
   "source": [
    "def build_year_calendar(year: int) -> pd.DataFrame:\n",
    "    \"\"\"Build calendar with Cyber Week and period assignments.\"\"\"\n",
    "    start = date(year, 1, 1)\n",
    "    end = date(year, 12, 31)\n",
    "    dates = pd.date_range(start, end, freq='D')\n",
    "    \n",
    "    cal = pd.DataFrame({'date': dates})\n",
    "    cal['day_of_week'] = cal['date'].dt.dayofweek\n",
    "    cal['week'] = cal['date'].dt.isocalendar().week\n",
    "    \n",
    "    # Cyber Week: week containing Black Friday (4th Thursday of November)\n",
    "    nov_days = [d for d in dates if d.month == 11]\n",
    "    thursdays = [d for d in nov_days if d.dayofweek == 3]\n",
    "    if len(thursdays) >= 4:\n",
    "        bf = thursdays[3]\n",
    "        cyber_week = bf.isocalendar()[1]\n",
    "        cal['is_cyber_week'] = cal['week'] == cyber_week\n",
    "    else:\n",
    "        cal['is_cyber_week'] = False\n",
    "    \n",
    "    # Period assignment (1-13)\n",
    "    cal['period'] = ((cal['date'].dt.dayofyear - 1) // 28) + 1\n",
    "    cal.loc[cal['period'] > 13, 'period'] = 13\n",
    "    \n",
    "    return cal\n",
    "\n",
    "# Test\n",
    "test_cal = build_year_calendar(2027)\n",
    "print(f\"Calendar test: {len(test_cal)} days in 2027\")\n",
    "print(f\"Cyber Week days: {test_cal['is_cyber_week'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac084dc3",
   "metadata": {},
   "source": [
    "### 2.2 Adoption Rate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec6ba612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market year 1: 0.0020\n",
      "Market year 3: 0.0050\n",
      "Market year 5: 0.0120\n",
      "Market year 8: 0.0320\n"
     ]
    }
   ],
   "source": [
    "def adoption_rate_by_scenario(market_year: int, scenario: str = 'mp') -> float:\n",
    "    \"\"\"\n",
    "    Returns adoption rate for a given market year under specified scenario.\n",
    "    market_year: 1-based (year 1 = entry year)\n",
    "    scenario: 'pes' (pessimistic), 'mp' (most probable), 'opt' (optimistic)\n",
    "    \"\"\"\n",
    "    scenario = scenario.lower().strip()\n",
    "    \n",
    "    rates = {\n",
    "        'pes': [0.001, 0.0015, 0.002, 0.003, 0.004, 0.006, 0.008, 0.010],\n",
    "        'mp':  [0.002, 0.003, 0.005, 0.008, 0.012, 0.018, 0.025, 0.032],\n",
    "        'opt': [0.003, 0.006, 0.012, 0.020, 0.032, 0.048, 0.064, 0.080],\n",
    "    }\n",
    "    \n",
    "    if scenario not in rates:\n",
    "        raise ValueError(f\"Invalid scenario: {scenario}\")\n",
    "    \n",
    "    idx = min(market_year - 1, len(rates[scenario]) - 1)\n",
    "    return rates[scenario][max(0, idx)]\n",
    "\n",
    "# Test\n",
    "for yr in [1, 3, 5, 8]:\n",
    "    print(f\"Market year {yr}: {adoption_rate_by_scenario(yr, 'mp'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb33dbf",
   "metadata": {},
   "source": [
    "### 2.3 Period Shares (Triangular Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52e11a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period shares shape: (3, 13)\n",
      "Sample shares (sim 0): [0.0834 0.0694 0.0884 0.0796 0.0514 0.0996 0.0827 0.084  0.054  0.0698\n",
      " 0.0667 0.0938 0.0772]\n",
      "Sum: 1.000000\n"
     ]
    }
   ],
   "source": [
    "def simulate_period_shares(n_sim: int = 1, seed: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate period shares (13 periods) using triangular distribution.\n",
    "    Returns: array of shape (n_sim, 13) with shares summing to 1.0\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    draws = rng.triangular(left=0.5, mode=1.0, right=1.5, size=(n_sim, 13))\n",
    "    return draws / draws.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Test\n",
    "test_shares = simulate_period_shares(n_sim=3, seed=42)\n",
    "print(f\"Period shares shape: {test_shares.shape}\")\n",
    "print(f\"Sample shares (sim 0): {test_shares[0].round(4)}\")\n",
    "print(f\"Sum: {test_shares[0].sum():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f82b5c",
   "metadata": {},
   "source": [
    "### 2.4 Model Shares (Triangular Variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a4db046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model shares function loaded\n"
     ]
    }
   ],
   "source": [
    "def triangular_model_shares(base_shares: np.ndarray, sim: int, year: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply triangular variation to base model shares.\n",
    "    Returns: array of 24 model shares summing to 1.0\n",
    "    \"\"\"\n",
    "    seed_val = sim * 10000 + year\n",
    "    rng = np.random.default_rng(seed_val)\n",
    "    draws = rng.triangular(left=0.8, mode=1.0, right=1.2, size=len(base_shares))\n",
    "    adjusted = base_shares * draws\n",
    "    return adjusted / adjusted.sum()\n",
    "\n",
    "# Test (will test after loading model_df)\n",
    "print(\"Model shares function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7855b",
   "metadata": {},
   "source": [
    "### 2.5 OTD Conversion Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25937bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metro, 0.5 days: 1.00\n",
      "Metro, 2.5 days: 0.85\n",
      "Non-Metro, 3.5 days: 0.85\n"
     ]
    }
   ],
   "source": [
    "def get_otd_conversion_rate(segment: str, otd_days: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate purchase probability based on OTD bucket.\n",
    "    segment: 'Metro' or 'Non-Metro'\n",
    "    otd_days: order-to-delivery time in days\n",
    "    \"\"\"\n",
    "    # Bucket OTD\n",
    "    if otd_days <= 1.0:\n",
    "        bucket = 0\n",
    "    elif otd_days <= 2.0:\n",
    "        bucket = 1\n",
    "    elif otd_days <= 3.0:\n",
    "        bucket = 2\n",
    "    else:\n",
    "        bucket = 3\n",
    "    \n",
    "    # Conversion rates by bucket and segment\n",
    "    conversion_matrix = {\n",
    "        'Metro': [1.00, 0.95, 0.85, 0.70],\n",
    "        'Non-Metro': [1.00, 0.98, 0.93, 0.85],\n",
    "    }\n",
    "    \n",
    "    return conversion_matrix.get(segment, [0.7]*4)[bucket]\n",
    "\n",
    "# Test\n",
    "print(f\"Metro, 0.5 days: {get_otd_conversion_rate('Metro', 0.5):.2f}\")\n",
    "print(f\"Metro, 2.5 days: {get_otd_conversion_rate('Metro', 2.5):.2f}\")\n",
    "print(f\"Non-Metro, 3.5 days: {get_otd_conversion_rate('Non-Metro', 3.5):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6da075",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ea0db",
   "metadata": {},
   "source": [
    "### 3.1 Load Population and Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc983518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ../Task4/Pop_inputs.xlsx\n",
      "  Metro cities: 273\n",
      "  Non-metro countries: 29\n",
      "  Product models: 24\n",
      "\n",
      "Model categories: {'Floor Care': 4, 'Kitchen Help': 4, 'Safety & Security': 4, 'Wall & Window': 4, 'Leisure': 4, 'Exterior Care': 4}\n",
      "Price range: €360 - €720\n",
      "\n",
      "Metro country_code sample:\n",
      "Country country_code      City  city_key\n",
      "Belgium           BE  Brussels  brussels\n",
      "Belgium           BE   Antwerp   antwerp\n",
      "Belgium           BE     Liege     liege\n",
      "Belgium           BE      Gent      gent\n",
      "Belgium           BE Charleroi charleroi\n"
     ]
    }
   ],
   "source": [
    "# Load BotWorld inputs\n",
    "input_file = TASK4_DIR / 'Pop_inputs.xlsx'\n",
    "\n",
    "print(f\"Loading data from: {input_file}\")\n",
    "\n",
    "# Metro cities with population projections\n",
    "metro_df = pd.read_excel(input_file, sheet_name='Metro')\n",
    "print(f\"  Metro cities: {len(metro_df)}\")\n",
    "\n",
    "# Non-metro population by country\n",
    "nonmetro_df = pd.read_excel(input_file, sheet_name='NonMetro')\n",
    "print(f\"  Non-metro countries: {len(nonmetro_df)}\")\n",
    "\n",
    "# 24 product models with prices and market shares\n",
    "model_df = pd.read_excel(input_file, sheet_name='Model')\n",
    "print(f\"  Product models: {len(model_df)}\")\n",
    "\n",
    "# ── Add country_code column (ISO 2-letter) to align with ENTRY_YEAR_MAP & assignment_df ──\n",
    "# Pop_inputs.xlsx uses full country names; all lookups use ISO codes\n",
    "metro_df['country_code'] = metro_df['Country'].map(COUNTRY_NAME_TO_CODE)\n",
    "nonmetro_df['country_code'] = nonmetro_df['Country'].map(COUNTRY_NAME_TO_CODE)\n",
    "\n",
    "unmapped_metro = metro_df[metro_df['country_code'].isna()]['Country'].unique()\n",
    "unmapped_nm = nonmetro_df[nonmetro_df['country_code'].isna()]['Country'].unique()\n",
    "if len(unmapped_metro) > 0:\n",
    "    print(f\"  WARNING - Metro countries not in COUNTRY_NAME_TO_CODE: {unmapped_metro}\")\n",
    "if len(unmapped_nm) > 0:\n",
    "    print(f\"  WARNING - NonMetro countries not in COUNTRY_NAME_TO_CODE: {unmapped_nm}\")\n",
    "\n",
    "# Normalize city name for OTD lookup: lowercase, spaces → underscores\n",
    "# (assignment_df node_ids use this format: e.g. 'frankfurt_am_main')\n",
    "metro_df['city_key'] = metro_df['City'].str.lower().str.replace(' ', '_', regex=False)\n",
    "\n",
    "# Prepare model data\n",
    "model_df['Share_MP'] = model_df['Share_MP'] / model_df['Share_MP'].sum()\n",
    "model_codes = model_df['Model'].values\n",
    "model_prices = model_df['Price_EUR'].values\n",
    "model_shares_base = model_df['Share_MP'].values\n",
    "\n",
    "print(f\"\\nModel categories: {model_df['Category'].value_counts().to_dict()}\")\n",
    "print(f\"Price range: €{model_prices.min():.0f} - €{model_prices.max():.0f}\")\n",
    "print(f\"\\nMetro country_code sample:\")\n",
    "print(metro_df[['Country', 'country_code', 'City', 'city_key']].head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3002dd",
   "metadata": {},
   "source": [
    "### 3.2 Load DC Assignment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c3fba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded assignment file: 1966 records\n",
      "  Years: [2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034]\n",
      "  Countries: 30\n",
      "  DCs: 4\n",
      "  Node types: {'metro': 1786, 'non_metro': 180}\n",
      "\n",
      "Sample assignments:\n",
      "   year             node_id  assigned_cand  units country segment\n",
      "0  2027   METRO_BE_brussels  CAND_DE_koeln    540      BE   Metro\n",
      "1  2027    METRO_BE_antwerp  CAND_DE_koeln    268      BE   Metro\n",
      "2  2027      METRO_BE_liege  CAND_DE_koeln    173      BE   Metro\n",
      "3  2027       METRO_BE_gent  CAND_DE_koeln    121      BE   Metro\n",
      "4  2027  METRO_BE_charleroi  CAND_DE_koeln    105      BE   Metro\n"
     ]
    }
   ],
   "source": [
    "# Load city-to-DC assignment from Task2\n",
    "assignment_file = TASK2_DIR / 'assignment_with_otd_prob_reachable.csv'\n",
    "assignment_df = pd.read_csv(assignment_file)\n",
    "\n",
    "print(f\"Loaded assignment file: {len(assignment_df)} records\")\n",
    "\n",
    "# Extract country and segment from node_id\n",
    "assignment_df['country'] = assignment_df['node_id'].str.extract(r'_(..?)_')[0]\n",
    "assignment_df['segment'] = assignment_df['node_type'].apply(\n",
    "    lambda x: 'Metro' if x == 'metro' else 'Non-Metro'\n",
    ")\n",
    "\n",
    "print(f\"  Years: {sorted(assignment_df['year'].unique())}\")\n",
    "print(f\"  Countries: {len(assignment_df['country'].unique())}\")\n",
    "print(f\"  DCs: {len(assignment_df['assigned_cand'].unique())}\")\n",
    "print(f\"  Node types: {assignment_df['node_type'].value_counts().to_dict()}\")\n",
    "\n",
    "# Sample\n",
    "print(\"\\nSample assignments:\")\n",
    "print(assignment_df[['year', 'node_id', 'assigned_cand', 'units', 'country', 'segment']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a82cf2",
   "metadata": {},
   "source": [
    "### 3.3 Prepare City Proportion Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b56c156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City proportions calculated: 1966 entries\n",
      "  Proportion sums - Min: 1.000000, Max: 1.000000\n",
      "\n",
      "Sample proportions (Germany Metro 2027):\n",
      "                   node_id  proportion assigned_cand\n",
      "           METRO_DE_berlin    0.143842 CAND_DE_koeln\n",
      "          METRO_DE_hamburg    0.071841 CAND_DE_koeln\n",
      "           METRO_DE_munich    0.064144 CAND_DE_koeln\n",
      "            METRO_DE_koeln    0.038807 CAND_DE_koeln\n",
      "METRO_DE_frankfurt_am_main    0.026620 CAND_DE_koeln\n",
      "      METRO_DE_duesseldorf    0.025818 CAND_DE_koeln\n",
      "        METRO_DE_stuttgart    0.025657 CAND_DE_koeln\n",
      "          METRO_DE_leipzig    0.024535 CAND_DE_koeln\n",
      "          METRO_DE_dresden    0.024054 CAND_DE_koeln\n",
      "         METRO_DE_dortmund    0.023733 CAND_DE_koeln\n"
     ]
    }
   ],
   "source": [
    "# Calculate city proportions within each (year, country, segment)\n",
    "# This will be used to disaggregate segment-level demand to city-level\n",
    "\n",
    "def calculate_city_proportions(assignment_df):\n",
    "    \"\"\"\n",
    "    For each (year, country, segment), calculate what proportion each city represents.\n",
    "    Returns: DataFrame with columns [year, country, segment, node_id, proportion, assigned_cand]\n",
    "    \"\"\"\n",
    "    # Calculate segment totals\n",
    "    segment_totals = assignment_df.groupby(['year', 'country', 'segment'])['units'].transform('sum')\n",
    "    \n",
    "    # Calculate proportions\n",
    "    proportions = assignment_df.copy()\n",
    "    proportions['proportion'] = proportions['units'] / segment_totals\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    proportions = proportions[['year', 'country', 'segment', 'node_id', 'proportion', 'assigned_cand', 'units']]\n",
    "    \n",
    "    return proportions\n",
    "\n",
    "city_proportions = calculate_city_proportions(assignment_df)\n",
    "\n",
    "print(f\"City proportions calculated: {len(city_proportions)} entries\")\n",
    "\n",
    "# Validation: proportions should sum to 1.0 within each segment\n",
    "validation = city_proportions.groupby(['year', 'country', 'segment'])['proportion'].sum()\n",
    "print(f\"  Proportion sums - Min: {validation.min():.6f}, Max: {validation.max():.6f}\")\n",
    "\n",
    "# Sample\n",
    "print(\"\\nSample proportions (Germany Metro 2027):\")\n",
    "sample = city_proportions[\n",
    "    (city_proportions['year'] == 2027) & \n",
    "    (city_proportions['country'] == 'DE') & \n",
    "    (city_proportions['segment'] == 'Metro')\n",
    "].head(10)\n",
    "print(sample[['node_id', 'proportion', 'assigned_cand']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f67777e",
   "metadata": {},
   "source": [
    "### 3.4 Load OTD Data (from Task2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a70518b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTD data loaded:\n",
      "  Metro cities with OTD: 272\n",
      "  Non-metro countries with OTD: 29\n",
      "  Non-metro countries: ['AT', 'BE', 'BG', 'CH', 'CY', 'CZ', 'DE', 'DK', 'EE', 'ES', 'FI', 'FR', 'GR', 'HR', 'HU', 'IE', 'IT', 'LT', 'LU', 'LV', 'MT', 'NL', 'NO', 'PL', 'PT', 'RO', 'SE', 'SI', 'SK']\n",
      "  Country-level metro OTD averages: 180 entries\n",
      "\n",
      "Sample metro_city_otd keys: ['brussels', 'antwerp', 'liege', 'gent', 'charleroi']\n",
      "Sample nonmetro_otd: {'BE': {2027: 1, 2028: 1, 2029: 1, 2030: 1, 2031: 1, 2032: 1, 2033: 1, 2034: 1}, 'DE': {2027: 2, 2028: 2, 2029: 2, 2030: 2, 2031: 2, 2032: 2, 2033: 2, 2034: 2}, 'LU': {2027: 1, 2028: 1, 2029: 1, 2030: 1, 2031: 1, 2032: 1, 2033: 1, 2034: 1}}\n"
     ]
    }
   ],
   "source": [
    "# Load OTD data from assignment file\n",
    "# node_id formats:\n",
    "#   Metro:    METRO_BE_brussels       → country = 'BE', city_key = 'brussels'\n",
    "#   NonMetro: NONMETRO_BE             → country = 'BE'  (no trailing city segment)\n",
    "# The original regex r'_(..?)_' fails for NONMETRO_BE (no second underscore)\n",
    "# Fix: extract second token by splitting on '_'\n",
    "\n",
    "metro_city_otd = {}     # {city_key: {year: otd_days}}   city_key = lowercase underscore\n",
    "nonmetro_otd = {}       # {country_code: {year: otd_days}}\n",
    "\n",
    "for _, row in assignment_df.iterrows():\n",
    "    year = row['year']\n",
    "    node_id = row['node_id']\n",
    "    otd_days = row['otd_days_promise']\n",
    "    tokens = node_id.split('_')   # e.g. ['METRO','BE','brussels'] or ['NONMETRO','BE']\n",
    "    country_code = tokens[1]      # always the second token\n",
    "\n",
    "    if row['node_type'] == 'metro':\n",
    "        # city_key = everything after 'METRO_CC_'\n",
    "        city_key = '_'.join(tokens[2:])   # handles multi-word cities like 'frankfurt_am_main'\n",
    "        if city_key not in metro_city_otd:\n",
    "            metro_city_otd[city_key] = {}\n",
    "        metro_city_otd[city_key][year] = otd_days\n",
    "    else:\n",
    "        if country_code not in nonmetro_otd:\n",
    "            nonmetro_otd[country_code] = {}\n",
    "        nonmetro_otd[country_code][year] = otd_days\n",
    "\n",
    "print(f\"OTD data loaded:\")\n",
    "print(f\"  Metro cities with OTD: {len(metro_city_otd)}\")\n",
    "print(f\"  Non-metro countries with OTD: {len(nonmetro_otd)}\")\n",
    "print(f\"  Non-metro countries: {sorted(nonmetro_otd.keys())}\")\n",
    "\n",
    "# For country-level population-weighted metro OTD averages (used as fallback)\n",
    "country_metro_otd_avg = (\n",
    "    assignment_df[assignment_df['node_type'] == 'metro']\n",
    "    .groupby(['year', 'country'])\n",
    "    .apply(\n",
    "        lambda x: (x['otd_days_promise'] * x['units']).sum() / x['units'].sum()\n",
    "        if x['units'].sum() > 0 else 2.0,\n",
    "        include_groups=False\n",
    "    )\n",
    "    .to_dict()\n",
    ")\n",
    "print(f\"  Country-level metro OTD averages: {len(country_metro_otd_avg)} entries\")\n",
    "\n",
    "# Spot check\n",
    "print(f\"\\nSample metro_city_otd keys: {list(metro_city_otd.keys())[:5]}\")\n",
    "print(f\"Sample nonmetro_otd: {dict(list(nonmetro_otd.items())[:3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480fce72",
   "metadata": {},
   "source": [
    "## 4. Modified Simulator Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcfd07b",
   "metadata": {},
   "source": [
    "**Key Modification**: Instead of aggregating to `(sim, date, model)`, \n",
    "we save `(sim, date, year, country, segment, model, sales_units)` \n",
    "to enable city-level disaggregation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5382008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_modified_simulator(\n",
    "    metro_df, nonmetro_df, model_df, entry_year_map, years,\n",
    "    metro_city_otd, nonmetro_otd, country_metro_otd_avg,\n",
    "    n_sim=100, seed=42, batch_size=10, out_dir=None, adoption_scenario='mp'\n",
    "):\n",
    "    \"\"\"\n",
    "    Modified OTD simulator that saves (country_code, segment) information.\n",
    "    \n",
    "    Key fixes vs original:\n",
    "    - Uses metro_df['country_code'] (ISO 2-letter) to match ENTRY_YEAR_MAP and city_proportions\n",
    "    - Uses metro_df['city_key'] (lowercase_underscore) to match metro_city_otd keys\n",
    "    - Uses nonmetro_otd with ISO country codes (fixed extraction in cell-23)\n",
    "    \n",
    "    Output per batch: (sim, date, year, country, segment, model, sales_units)\n",
    "    where 'country' is ISO 2-letter code to align with city_proportions merge.\n",
    "    \"\"\"\n",
    "    adoption_scenario = adoption_scenario.lower().strip()\n",
    "    if adoption_scenario not in {'pes', 'mp', 'opt'}:\n",
    "        raise ValueError(f\"adoption_scenario must be one of {{'pes','mp','opt'}}, got '{adoption_scenario}'\")\n",
    "    \n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    rng_global = np.random.default_rng(seed)\n",
    "    model_shares_base = model_df['Share_MP'].values\n",
    "    model_codes = model_df['Model'].values\n",
    "    model_prices = model_df['Price_EUR'].values\n",
    "    \n",
    "    sim_records = []\n",
    "    annual_summary = []\n",
    "    segment_summary = []\n",
    "    \n",
    "    for sim in range(n_sim):\n",
    "        print(f'  Sim {sim+1}/{n_sim}...', end=' ', flush=True)\n",
    "        \n",
    "        for yr in years:\n",
    "            cal = build_year_calendar(yr)\n",
    "            cw_days = cal[cal['is_cyber_week']]['date'].values\n",
    "            ncw_days = cal[~cal['is_cyber_week']]\n",
    "            n_cw = len(cw_days)\n",
    "            \n",
    "            # ── 1. Annual demand by country_code-segment ──\n",
    "            segment_annual = {}\n",
    "            \n",
    "            for _, city_row in metro_df.iterrows():\n",
    "                country = city_row['country_code']   # ISO 2-letter code\n",
    "                if pd.isna(country) or country not in entry_year_map:\n",
    "                    continue\n",
    "                market_year = yr - entry_year_map[country] + 1\n",
    "                if market_year < 1:\n",
    "                    continue\n",
    "                pop = city_row[f'Pop_{yr}']\n",
    "                rate = adoption_rate_by_scenario(market_year, adoption_scenario)\n",
    "                key = (country, 'Metro')\n",
    "                segment_annual[key] = segment_annual.get(key, 0) + pop * rate\n",
    "            \n",
    "            for _, nm_row in nonmetro_df.iterrows():\n",
    "                country = nm_row['country_code']     # ISO 2-letter code\n",
    "                if pd.isna(country) or country not in entry_year_map:\n",
    "                    continue\n",
    "                market_year = yr - entry_year_map[country] + 1\n",
    "                if market_year < 1:\n",
    "                    continue\n",
    "                pop = nm_row[f'NonMetroPop_{yr}']\n",
    "                rate = adoption_rate_by_scenario(market_year, adoption_scenario)\n",
    "                segment_annual[(country, 'Non-Metro')] = pop * rate\n",
    "            \n",
    "            if not segment_annual:\n",
    "                continue\n",
    "            total_annual = sum(segment_annual.values())\n",
    "            if total_annual == 0:\n",
    "                continue\n",
    "            \n",
    "            # ── 2. Period shares ──\n",
    "            period_shares = simulate_period_shares(\n",
    "                n_sim=1, seed=int(rng_global.integers(0, 1_000_000))\n",
    "            )[0]\n",
    "            \n",
    "            # ── 3. Cyber vs regular split ──\n",
    "            cyber_total = total_annual * CYBER_WEEK_SHARE\n",
    "            regular_total = total_annual * (1.0 - CYBER_WEEK_SHARE)\n",
    "            \n",
    "            # ── 4. Daily demand allocation ──\n",
    "            day_demand_total = {}\n",
    "            for p in range(1, 14):\n",
    "                period_units = regular_total * period_shares[p - 1]\n",
    "                p_days = ncw_days[ncw_days['period'] == p]\n",
    "                if len(p_days) == 0:\n",
    "                    continue\n",
    "                dow_w = p_days['day_of_week'].map(lambda d: DOW_WEIGHTS[d]).values\n",
    "                dow_w = dow_w / dow_w.sum()\n",
    "                for d_idx, (_, day_row) in enumerate(p_days.iterrows()):\n",
    "                    day_demand_total[day_row['date']] = period_units * dow_w[d_idx]\n",
    "            \n",
    "            if n_cw > 0:\n",
    "                cw_cal = cal[cal['is_cyber_week']]\n",
    "                cw_dow_w = cw_cal['day_of_week'].map(lambda d: DOW_WEIGHTS[d]).values\n",
    "                cw_dow_w = cw_dow_w / cw_dow_w.sum()\n",
    "                for i, (_, cw_row) in enumerate(cw_cal.iterrows()):\n",
    "                    day_demand_total[cw_row['date']] = cyber_total * cw_dow_w[i]\n",
    "            \n",
    "            # ── 5. Segment weights & OTD ──\n",
    "            segment_weights = {k: v / total_annual for k, v in segment_annual.items()}\n",
    "            cw_date_set = {pd.Timestamp(x).date() if not isinstance(x, date) else x\n",
    "                          for x in cw_days}\n",
    "            \n",
    "            # Country-level population-weighted metro OTD for the year\n",
    "            # Uses city_key (lowercase_underscore) to match metro_city_otd keys\n",
    "            country_metro_otd_yr = {}\n",
    "            for _, city_row in metro_df.iterrows():\n",
    "                country = city_row['country_code']\n",
    "                if pd.isna(country):\n",
    "                    continue\n",
    "                city_key = city_row['city_key']       # normalized: 'frankfurt_am_main'\n",
    "                pop = city_row[f'Pop_{yr}']\n",
    "                city_otd = metro_city_otd.get(city_key, {}).get(yr, 2.0)\n",
    "                if country not in country_metro_otd_yr:\n",
    "                    country_metro_otd_yr[country] = {'weighted_sum': 0.0, 'total_pop': 0.0}\n",
    "                country_metro_otd_yr[country]['weighted_sum'] += city_otd * pop\n",
    "                country_metro_otd_yr[country]['total_pop'] += pop\n",
    "            \n",
    "            for country in country_metro_otd_yr:\n",
    "                tp = country_metro_otd_yr[country]['total_pop']\n",
    "                country_metro_otd_yr[country] = (\n",
    "                    country_metro_otd_yr[country]['weighted_sum'] / tp if tp > 0 else 2.0\n",
    "                )\n",
    "            \n",
    "            # ── 6. OTD conversion at segment level ──\n",
    "            daily_segment_sales = {}\n",
    "            for d, total_units in day_demand_total.items():\n",
    "                is_cw = d in cw_date_set\n",
    "                for (country, segment), seg_weight in segment_weights.items():\n",
    "                    seg_demand = total_units * seg_weight\n",
    "                    if segment == 'Metro':\n",
    "                        otd_days = country_metro_otd_yr.get(country, 2.0)\n",
    "                    else:\n",
    "                        otd_days = nonmetro_otd.get(country, {}).get(yr, 5.0)\n",
    "                    conversion_rate = get_otd_conversion_rate(segment, otd_days)\n",
    "                    daily_segment_sales[(d, country, segment, is_cw)] = {\n",
    "                        'demand': seg_demand,\n",
    "                        'sales': seg_demand * conversion_rate,\n",
    "                        'otd_days': otd_days,\n",
    "                        'conversion_rate': conversion_rate,\n",
    "                    }\n",
    "            \n",
    "            # ── 7. Decompose to 24 products ──\n",
    "            model_shares = triangular_model_shares(model_shares_base, sim, yr)\n",
    "            for (d, country, segment, is_cw), agg in daily_segment_sales.items():\n",
    "                ms_vec = agg['sales'] * model_shares\n",
    "                md_vec = agg['demand'] * model_shares\n",
    "                p_factor = (1.0 - CYBER_PRICE_DISCOUNT) if is_cw else 1.0\n",
    "                for m_idx, mdl in enumerate(model_codes):\n",
    "                    ms = ms_vec[m_idx]\n",
    "                    if ms < 1e-9:\n",
    "                        continue\n",
    "                    sim_records.append({\n",
    "                        'sim': sim,\n",
    "                        'date': pd.Timestamp(d),\n",
    "                        'year': yr,\n",
    "                        'country': country,       # ISO 2-letter code\n",
    "                        'segment': segment,\n",
    "                        'is_cyber_week': is_cw,\n",
    "                        'demand_units': md_vec[m_idx],\n",
    "                        'otd_days': agg['otd_days'],\n",
    "                        'conversion_rate': agg['conversion_rate'],\n",
    "                        'sales_units': ms,\n",
    "                        'model': mdl,\n",
    "                        'revenue': ms * model_prices[m_idx] * p_factor,\n",
    "                    })\n",
    "        \n",
    "        print('Done')\n",
    "        \n",
    "        # ── Batch flush with country & segment preserved ──\n",
    "        if batch_size and out_dir and (sim + 1) % batch_size == 0:\n",
    "            batch_idx = (sim + 1) // batch_size - 1\n",
    "            batch_df = pd.DataFrame(sim_records)\n",
    "            \n",
    "            # Save (sim, date, year, country, segment, model, sales_units)\n",
    "            # country is ISO 2-letter code → aligns with city_proportions merge\n",
    "            segment_batch = (\n",
    "                batch_df[['sim', 'date', 'year', 'country', 'segment', 'model', 'sales_units']]\n",
    "                .groupby(['sim', 'date', 'year', 'country', 'segment', 'model'], as_index=False)\n",
    "                .agg(sales_units=('sales_units', 'sum'))\n",
    "            )\n",
    "            \n",
    "            out_path = os.path.join(out_dir, f'batch_{batch_idx:02d}.csv.gz')\n",
    "            segment_batch.to_csv(out_path, index=False, compression='gzip')\n",
    "            print(f'    → Saved batch {batch_idx} to {out_path}')\n",
    "            \n",
    "            # Annual summary\n",
    "            annual_summary.append(\n",
    "                batch_df.groupby(['sim', 'year']).agg(\n",
    "                    demand_units=('demand_units', 'sum'),\n",
    "                    sales_units=('sales_units', 'sum'),\n",
    "                    revenue=('revenue', 'sum'),\n",
    "                ).reset_index()\n",
    "            )\n",
    "            \n",
    "            # Segment summary\n",
    "            segment_summary.append(\n",
    "                batch_df.groupby(['sim', 'year', 'country', 'segment']).agg(\n",
    "                    demand_units=('demand_units', 'sum'),\n",
    "                    sales_units=('sales_units', 'sum'),\n",
    "                    otd_days=('otd_days', 'mean'),\n",
    "                ).reset_index()\n",
    "            )\n",
    "            \n",
    "            del batch_df, segment_batch\n",
    "            sim_records = []\n",
    "            gc.collect()\n",
    "    \n",
    "    if annual_summary:\n",
    "        annual_df = pd.concat(annual_summary, ignore_index=True)\n",
    "        segment_df = pd.concat(segment_summary, ignore_index=True)\n",
    "        return {'annual': annual_df, 'segment': segment_df}\n",
    "    else:\n",
    "        return pd.DataFrame(sim_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c692003",
   "metadata": {},
   "source": [
    "## 5. City-Level Disaggregation & DC Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d337c1",
   "metadata": {},
   "source": [
    "### 5.1 Disaggregate Segment-Level to City-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c21ae7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disaggregate_to_city_level(segment_df, city_proportions):\n",
    "    \"\"\"\n",
    "    Disaggregate segment-level demand to city-level using population proportions.\n",
    "    \n",
    "    Input: (sim, date, year, country, segment, model, sales_units)\n",
    "    Output: (sim, date, year, node_id, euro_dc_id, model, city_demand)\n",
    "    \"\"\"\n",
    "    print(\"\\nDisaggregating segment-level demand to city-level...\")\n",
    "    \n",
    "    # Merge with city proportions\n",
    "    merged = segment_df.merge(\n",
    "        city_proportions,\n",
    "        on=['year', 'country', 'segment'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Calculate city-level demand\n",
    "    merged['city_demand'] = merged['sales_units'] * merged['proportion']\n",
    "    \n",
    "    # Select final columns\n",
    "    city_level = merged[[\n",
    "        'sim', 'date', 'year', 'node_id', 'assigned_cand', 'model', 'city_demand'\n",
    "    ]].copy()\n",
    "    \n",
    "    city_level.rename(columns={'assigned_cand': 'euro_dc_id'}, inplace=True)\n",
    "    \n",
    "    print(f\"  Input records: {len(segment_df):,}\")\n",
    "    print(f\"  Output records: {len(city_level):,}\")\n",
    "    print(f\"  Expansion factor: {len(city_level)/len(segment_df):.1f}x\")\n",
    "    \n",
    "    # Validation: total demand should be conserved\n",
    "    total_in = segment_df['sales_units'].sum()\n",
    "    total_out = city_level['city_demand'].sum()\n",
    "    print(f\"  Demand conservation: {total_out/total_in*100:.2f}%\")\n",
    "    \n",
    "    return city_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97630eb1",
   "metadata": {},
   "source": [
    "### 5.2 Aggregate City-Level to DC-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "824de20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_dc_level(city_df):\n",
    "    \"\"\"\n",
    "    Aggregate city-level demand to DC-level.\n",
    "    \n",
    "    Input: (sim, date, year, node_id, euro_dc_id, model, city_demand)\n",
    "    Output: (sim, date, year, euro_dc_id, model, realized_units)\n",
    "    \"\"\"\n",
    "    print(\"\\nAggregating city-level demand to DC-level...\")\n",
    "    \n",
    "    dc_level = city_df.groupby(\n",
    "        ['sim', 'date', 'year', 'euro_dc_id', 'model'],\n",
    "        as_index=False\n",
    "    ).agg(realized_units=('city_demand', 'sum'))\n",
    "    \n",
    "    print(f\"  Input records: {len(city_df):,}\")\n",
    "    print(f\"  Output records: {len(dc_level):,}\")\n",
    "    print(f\"  Reduction factor: {len(city_df)/len(dc_level):.1f}x\")\n",
    "    \n",
    "    # Validation\n",
    "    total_in = city_df['city_demand'].sum()\n",
    "    total_out = dc_level['realized_units'].sum()\n",
    "    print(f\"  Demand conservation: {total_out/total_in*100:.2f}%\")\n",
    "    \n",
    "    return dc_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6dc69",
   "metadata": {},
   "source": [
    "## 6. Main Execution Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329c596",
   "metadata": {},
   "source": [
    "This section runs the complete pipeline:\n",
    "1. Run modified simulator → Save batches with (country, segment)\n",
    "2. Load all batches\n",
    "3. Disaggregate to city-level\n",
    "4. Aggregate to DC-level\n",
    "5. Save final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ce67476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 1: Running Modified Simulator\n",
      "======================================================================\n",
      "Configuration:\n",
      "  Simulations: 100\n",
      "  Years: 2027-2034\n",
      "  Adoption: mp\n",
      "  Batch size: 10\n",
      "\n",
      "  Sim 1/100... Done\n",
      "  Sim 2/100... Done\n",
      "  Sim 3/100... Done\n",
      "  Sim 4/100... Done\n",
      "  Sim 5/100... Done\n",
      "  Sim 6/100... Done\n",
      "  Sim 7/100... Done\n",
      "  Sim 8/100... Done\n",
      "  Sim 9/100... Done\n",
      "  Sim 10/100... Done\n",
      "    → Saved batch 0 to ../Task4/dc_output/batch_00.csv.gz\n",
      "  Sim 11/100... Done\n",
      "  Sim 12/100... Done\n",
      "  Sim 13/100... Done\n",
      "  Sim 14/100... Done\n",
      "  Sim 15/100... Done\n",
      "  Sim 16/100... Done\n",
      "  Sim 17/100... Done\n",
      "  Sim 18/100... Done\n",
      "  Sim 19/100... Done\n",
      "  Sim 20/100... Done\n",
      "    → Saved batch 1 to ../Task4/dc_output/batch_01.csv.gz\n",
      "  Sim 21/100... Done\n",
      "  Sim 22/100... Done\n",
      "  Sim 23/100... Done\n",
      "  Sim 24/100... Done\n",
      "  Sim 25/100... Done\n",
      "  Sim 26/100... Done\n",
      "  Sim 27/100... Done\n",
      "  Sim 28/100... Done\n",
      "  Sim 29/100... Done\n",
      "  Sim 30/100... Done\n",
      "    → Saved batch 2 to ../Task4/dc_output/batch_02.csv.gz\n",
      "  Sim 31/100... Done\n",
      "  Sim 32/100... Done\n",
      "  Sim 33/100... Done\n",
      "  Sim 34/100... Done\n",
      "  Sim 35/100... Done\n",
      "  Sim 36/100... Done\n",
      "  Sim 37/100... Done\n",
      "  Sim 38/100... Done\n",
      "  Sim 39/100... Done\n",
      "  Sim 40/100... Done\n",
      "    → Saved batch 3 to ../Task4/dc_output/batch_03.csv.gz\n",
      "  Sim 41/100... Done\n",
      "  Sim 42/100... Done\n",
      "  Sim 43/100... Done\n",
      "  Sim 44/100... Done\n",
      "  Sim 45/100... Done\n",
      "  Sim 46/100... Done\n",
      "  Sim 47/100... Done\n",
      "  Sim 48/100... Done\n",
      "  Sim 49/100... Done\n",
      "  Sim 50/100... Done\n",
      "    → Saved batch 4 to ../Task4/dc_output/batch_04.csv.gz\n",
      "  Sim 51/100... Done\n",
      "  Sim 52/100... Done\n",
      "  Sim 53/100... Done\n",
      "  Sim 54/100... Done\n",
      "  Sim 55/100... Done\n",
      "  Sim 56/100... Done\n",
      "  Sim 57/100... Done\n",
      "  Sim 58/100... Done\n",
      "  Sim 59/100... Done\n",
      "  Sim 60/100... Done\n",
      "    → Saved batch 5 to ../Task4/dc_output/batch_05.csv.gz\n",
      "  Sim 61/100... Done\n",
      "  Sim 62/100... Done\n",
      "  Sim 63/100... Done\n",
      "  Sim 64/100... Done\n",
      "  Sim 65/100... Done\n",
      "  Sim 66/100... Done\n",
      "  Sim 67/100... Done\n",
      "  Sim 68/100... Done\n",
      "  Sim 69/100... Done\n",
      "  Sim 70/100... Done\n",
      "    → Saved batch 6 to ../Task4/dc_output/batch_06.csv.gz\n",
      "  Sim 71/100... Done\n",
      "  Sim 72/100... Done\n",
      "  Sim 73/100... Done\n",
      "  Sim 74/100... Done\n",
      "  Sim 75/100... Done\n",
      "  Sim 76/100... Done\n",
      "  Sim 77/100... Done\n",
      "  Sim 78/100... Done\n",
      "  Sim 79/100... Done\n",
      "  Sim 80/100... Done\n",
      "    → Saved batch 7 to ../Task4/dc_output/batch_07.csv.gz\n",
      "  Sim 81/100... Done\n",
      "  Sim 82/100... Done\n",
      "  Sim 83/100... Done\n",
      "  Sim 84/100... Done\n",
      "  Sim 85/100... Done\n",
      "  Sim 86/100... Done\n",
      "  Sim 87/100... Done\n",
      "  Sim 88/100... Done\n",
      "  Sim 89/100... Done\n",
      "  Sim 90/100... Done\n",
      "    → Saved batch 8 to ../Task4/dc_output/batch_08.csv.gz\n",
      "  Sim 91/100... Done\n",
      "  Sim 92/100... Done\n",
      "  Sim 93/100... Done\n",
      "  Sim 94/100... Done\n",
      "  Sim 95/100... Done\n",
      "  Sim 96/100... Done\n",
      "  Sim 97/100... Done\n",
      "  Sim 98/100... Done\n",
      "  Sim 99/100... Done\n",
      "  Sim 100/100... Done\n",
      "    → Saved batch 9 to ../Task4/dc_output/batch_09.csv.gz\n",
      "\n",
      "======================================================================\n",
      "Simulation Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# STEP 1: Run Modified Simulator\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: Running Modified Simulator\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Simulations: {N_SIM}\")\n",
    "print(f\"  Years: {YEARS[0]}-{YEARS[-1]}\")\n",
    "print(f\"  Adoption: {ADOPTION_SCENARIO}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print()\n",
    "\n",
    "summary = run_modified_simulator(\n",
    "    metro_df=metro_df,\n",
    "    nonmetro_df=nonmetro_df,\n",
    "    model_df=model_df,\n",
    "    entry_year_map=ENTRY_YEAR_MAP,\n",
    "    years=YEARS,\n",
    "    metro_city_otd=metro_city_otd,\n",
    "    nonmetro_otd=nonmetro_otd,\n",
    "    country_metro_otd_avg=country_metro_otd_avg,\n",
    "    n_sim=N_SIM,\n",
    "    seed=SEED,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    out_dir=str(OUTPUT_DIR),\n",
    "    adoption_scenario=ADOPTION_SCENARIO\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Simulation Complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: Loading All Batch Files\n",
      "======================================================================\n",
      "Found 10 batch files\n",
      "  Loaded batch_00.csv.gz: 27,348,000 records\n",
      "  Loaded batch_01.csv.gz: 27,348,000 records\n",
      "  Loaded batch_02.csv.gz: 27,348,000 records\n",
      "  Loaded batch_03.csv.gz: 27,348,000 records\n",
      "  Loaded batch_04.csv.gz: 27,348,000 records\n",
      "  Loaded batch_05.csv.gz: 27,348,000 records\n",
      "  Loaded batch_06.csv.gz: 27,348,000 records\n",
      "  Loaded batch_07.csv.gz: 27,348,000 records\n",
      "  Loaded batch_08.csv.gz: 27,348,000 records\n",
      "  Loaded batch_09.csv.gz: 27,348,000 records\n",
      "\n",
      "Total records: 273,480,000\n",
      "Columns: ['sim', 'date', 'year', 'country', 'segment', 'model', 'sales_units']\n",
      "\n",
      "Sample:\n",
      "   sim        date  year country segment model  sales_units\n",
      "0    0  2027-01-01  2027      BE   Metro   F10     3.506399\n",
      "1    0  2027-01-01  2027      BE   Metro   F20     1.978292\n",
      "2    0  2027-01-01  2027      BE   Metro   F30     0.826472\n",
      "3    0  2027-01-01  2027      BE   Metro   F50     0.525470\n",
      "4    0  2027-01-01  2027      BE   Metro   K10     3.169982\n",
      "5    0  2027-01-01  2027      BE   Metro   K20     1.569216\n",
      "6    0  2027-01-01  2027      BE   Metro   K30     0.730918\n",
      "7    0  2027-01-01  2027      BE   Metro   K50     0.460463\n",
      "8    0  2027-01-01  2027      BE   Metro   L10     0.444987\n",
      "9    0  2027-01-01  2027      BE   Metro   L20     1.192541\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# STEP 2: Load All Batch Files\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: Loading All Batch Files\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "batch_files = sorted(OUTPUT_DIR.glob('batch_*.csv.gz'))\n",
    "print(f\"Found {len(batch_files)} batch files\")\n",
    "\n",
    "all_batches = []\n",
    "for batch_file in batch_files:\n",
    "    batch_df = pd.read_csv(batch_file)\n",
    "    all_batches.append(batch_df)\n",
    "    print(f\"  Loaded {batch_file.name}: {len(batch_df):,} records\")\n",
    "\n",
    "segment_output = pd.concat(all_batches, ignore_index=True)\n",
    "print(f\"\\nTotal records: {len(segment_output):,}\")\n",
    "print(f\"Columns: {list(segment_output.columns)}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(segment_output.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: Disaggregating to City-Level\n",
      "======================================================================\n",
      "\n",
      "Disaggregating segment-level demand to city-level...\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# STEP 3: Disaggregate to City-Level\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: Disaggregating to City-Level\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "city_output = disaggregate_to_city_level(segment_output, city_proportions)\n",
    "\n",
    "print(f\"\\nCity-level output shape: {city_output.shape}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(city_output.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# STEP 4: Aggregate to DC-Level\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: Aggregating to DC-Level\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dc_output = aggregate_to_dc_level(city_output)\n",
    "\n",
    "print(f\"\\nDC-level output shape: {dc_output.shape}\")\n",
    "print(f\"\\nColumns: {list(dc_output.columns)}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(dc_output.head(20))\n",
    "\n",
    "# Clean up memory\n",
    "del segment_output, city_output\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# STEP 5: Save Final Output\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 5: Saving Final Output\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "output_file = OUTPUT_DIR / 'dc_daily_demand.csv.gz'\n",
    "dc_output.to_csv(output_file, index=False, compression='gzip')\n",
    "\n",
    "print(f\"Saved to: {output_file}\")\n",
    "print(f\"File size: {output_file.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Total records: {len(dc_output):,}\")\n",
    "print(\"\\nOutput schema: (sim, date, year, euro_dc_id, model, realized_units)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation & Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION & SANITY CHECKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check 1: Simulation coverage\n",
    "print(\"\\n1. Simulation Coverage\")\n",
    "print(\"-\" * 70)\n",
    "sims = dc_output['sim'].unique()\n",
    "print(f\"  Number of simulations: {len(sims)}\")\n",
    "print(f\"  Expected: {N_SIM}\")\n",
    "print(f\"  Range: {sims.min()} - {sims.max()}\")\n",
    "assert len(sims) == N_SIM, \"Missing simulations!\"\n",
    "\n",
    "# Check 2: Year coverage\n",
    "print(\"\\n2. Year Coverage\")\n",
    "print(\"-\" * 70)\n",
    "years = sorted(dc_output['year'].unique())\n",
    "print(f\"  Years present: {years}\")\n",
    "print(f\"  Expected: {YEARS}\")\n",
    "assert years == YEARS, \"Year mismatch!\"\n",
    "\n",
    "# Check 3: Date range\n",
    "print(\"\\n3. Date Range\")\n",
    "print(\"-\" * 70)\n",
    "min_date = dc_output['date'].min()\n",
    "max_date = dc_output['date'].max()\n",
    "print(f\"  Min date: {min_date}\")\n",
    "print(f\"  Max date: {max_date}\")\n",
    "print(f\"  Total days: {(pd.to_datetime(max_date) - pd.to_datetime(min_date)).days + 1}\")\n",
    "\n",
    "# Check 4: DC coverage\n",
    "print(\"\\n4. DC Coverage\")\n",
    "print(\"-\" * 70)\n",
    "dcs = dc_output['euro_dc_id'].unique()\n",
    "print(f\"  Number of DCs: {len(dcs)}\")\n",
    "print(f\"  DCs: {sorted(dcs)}\")\n",
    "\n",
    "# Check 5: Model coverage\n",
    "print(\"\\n5. Model Coverage\")\n",
    "print(\"-\" * 70)\n",
    "models = sorted(dc_output['model'].unique())\n",
    "print(f\"  Number of models: {len(models)}\")\n",
    "print(f\"  Expected: 24\")\n",
    "print(f\"  Models: {models}\")\n",
    "assert len(models) == 24, \"Missing models!\"\n",
    "\n",
    "# Check 6: Realized units statistics\n",
    "print(\"\\n6. Realized Units Statistics\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Total realized units: {dc_output['realized_units'].sum():,.0f}\")\n",
    "print(f\"  Mean per record: {dc_output['realized_units'].mean():.2f}\")\n",
    "print(f\"  Median per record: {dc_output['realized_units'].median():.2f}\")\n",
    "print(f\"  Min: {dc_output['realized_units'].min():.6f}\")\n",
    "print(f\"  Max: {dc_output['realized_units'].max():.2f}\")\n",
    "\n",
    "# Check 7: Null values\n",
    "print(\"\\n7. Null Value Check\")\n",
    "print(\"-\" * 70)\n",
    "nulls = dc_output.isnull().sum()\n",
    "print(nulls)\n",
    "assert nulls.sum() == 0, \"Null values found!\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ ALL VALIDATION CHECKS PASSED\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Aggregate by DC\n",
    "print(\"\\n1. Total Demand by DC (across all sims)\")\n",
    "print(\"-\" * 70)\n",
    "dc_totals = dc_output.groupby('euro_dc_id')['realized_units'].sum().sort_values(ascending=False)\n",
    "print(dc_totals)\n",
    "print(f\"\\nTotal across all DCs: {dc_totals.sum():,.0f}\")\n",
    "\n",
    "# Aggregate by year\n",
    "print(\"\\n2. Total Demand by Year (across all sims)\")\n",
    "print(\"-\" * 70)\n",
    "year_totals = dc_output.groupby('year')['realized_units'].sum() / N_SIM\n",
    "print(year_totals)\n",
    "\n",
    "# Aggregate by model category\n",
    "print(\"\\n3. Total Demand by Model (top 10)\")\n",
    "print(\"-\" * 70)\n",
    "model_totals = dc_output.groupby('model')['realized_units'].sum().sort_values(ascending=False)\n",
    "print(model_totals.head(10))\n",
    "\n",
    "# Average daily demand by DC\n",
    "print(\"\\n4. Average Daily Demand by DC (per sim)\")\n",
    "print(\"-\" * 70)\n",
    "dc_daily_avg = dc_output.groupby(['sim', 'date', 'euro_dc_id'])['realized_units'].sum().groupby('euro_dc_id').mean()\n",
    "print(dc_daily_avg.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sample Visualizations (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Uncomment if matplotlib is available\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot 1: Total demand over time (average across sims)\n",
    "# daily_demand = dc_output.groupby('date')['realized_units'].sum() / N_SIM\n",
    "# \n",
    "# plt.figure(figsize=(14, 6))\n",
    "# plt.plot(daily_demand.index, daily_demand.values)\n",
    "# plt.title('Daily Total Realized Demand (Average across simulations)')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Realized Units')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(OUTPUT_DIR / 'daily_demand.png', dpi=150)\n",
    "# print(\"Saved: daily_demand.png\")\n",
    "# \n",
    "# # Plot 2: Demand by DC over years\n",
    "# dc_yearly = dc_output.groupby(['year', 'euro_dc_id'])['realized_units'].sum() / N_SIM\n",
    "# dc_yearly_pivot = dc_yearly.unstack(fill_value=0)\n",
    "# \n",
    "# plt.figure(figsize=(12, 8))\n",
    "# dc_yearly_pivot.T.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "# plt.title('Annual Demand by DC (Average across simulations)')\n",
    "# plt.xlabel('DC')\n",
    "# plt.ylabel('Realized Units')\n",
    "# plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(OUTPUT_DIR / 'dc_yearly_demand.png', dpi=150)\n",
    "# print(\"Saved: dc_yearly_demand.png\")\n",
    "\n",
    "print(\"Visualization section (commented out by default)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TASK 4 COMPLETE - DC DEMAND SIMULATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n✓ Successfully generated DC-level daily demand\")\n",
    "print(f\"\\nOutput file: {output_file}\")\n",
    "print(f\"  - Simulations: {N_SIM}\")\n",
    "print(f\"  - Years: {YEARS[0]}-{YEARS[-1]}\")\n",
    "print(f\"  - DCs: {len(dcs)}\")\n",
    "print(f\"  - Models: {len(models)}\")\n",
    "print(f\"  - Total records: {len(dc_output):,}\")\n",
    "print(f\"  - File size: {output_file.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print(f\"\\nSchema: (sim, date, year, euro_dc_id, model, realized_units)\")\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Load dc_daily_demand.csv.gz for further analysis\")\n",
    "print(f\"  2. Use for capacity planning, inventory management, etc.\")\n",
    "print(f\"  3. Aggregate as needed for different planning horizons\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "General_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
